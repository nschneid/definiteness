\PassOptionsToPackage{usenames}{color}
\documentclass[11pt,letterpaper]{article}
\usepackage{comment}
\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{colingstyle/coling2014}

\usepackage[linkcolor=blue]{hyperref}

\usepackage[round]{natbib}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})}

%\usepackage{times}
%\usepackage{latexsym}


\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage[procnames]{listings}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

\usepackage{mathptmx}	% txfonts
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}





% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}


\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
\usepackage{tikz}
%\usepackage{tree-dvips}
\usetikzlibrary{arrows,positioning,calc} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{verbatim}
\usepackage{ulem}


% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
\makeatletter
\lst@AddToHook{EveryPar}{%
  \label{lst:\thelstnumber}% make a label for each line number except the first (assumes only one listing in the document)
}
\makeatother
\lstset{
% basicstyle=\rmshape,
  numbers=left,
  numberstyle=\tt\color{gray},
  firstnumber=2,
  stepnumber=5,
  xleftmargin=3em,
  language=Python,
  upquote=true,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=1,
  stringstyle=\color{mdgreen},
  commentstyle=\itshape\color{lavender},
  basicstyle=\small\smaller\ttfamily,
  morekeywords={lambda,with,as,assert},
  keywordstyle=\bfseries\color{magenta},
  procnamekeys={def},
  procnamestyle=\bfseries\color{orange},
  aboveskip=0.5cm,
  belowskip=0.5cm
}
\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\abmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{A}}_{\textsc{B}}}}}}
\newcommand{\ytmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{Y}}_{\textsc{T}}}}}}
\newcommand{\ftamarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{FT}}_{\textsc{A}}}}}}
\newcommand{\lslmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{LS}}_{\textsc{L}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\ab}[1]{\arkcomment{\abmarker}{#1}{red}}
\newcommand{\yt}[1]{\arkcomment{\ytmarker}{#1}{blue}}
\newcommand{\fta}[1]{\arkcomment{\ftamarker}{#1}{orange}}
\newcommand{\lsl}[1]{\arkcomment{\lslmarker}{#1}{purple}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\usepackage{cleveref}

% use \S for all references to all kinds of sections, and \P to paragraphs
% (sadly, we cannot use the simpler \crefname{} macro because it would insert a space after the symbol)
\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
\crefmultiformat{part}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{chapter}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{section}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsubsection}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{part}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{chapter}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{section}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsubsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{paragraph}{\mbox{\P\P#3#1#4--#5#2#6}}
\crefrangeformat{subparagraph}{\mbox{\P\P#3#1#4--#5#2#6}}
% for \label[appsec]{...}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{enums,enumsi}{example}{examples}
\Crefname{enums,enumsi}{Example}{Examples}
\crefformat{enums}{(#2#1#3)}
\crefformat{enumsi}{(#2#1#3)}
\crefrangeformat{enums}{\mbox{(#3#1#4--#5#2#6)}}
\crefrangeformat{enumsi}{\mbox{(#3#1#4--#5#2#6)}}

\ifx\creflastconjunction\undefined%
\newcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\else%
\renewcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\fi%

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
\newcommand{\fnref}[1]{\autoref{#1}} % don't use \cref{} due to bug in (now out-of-date) cleveref package w.r.t. footnotes


% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
%\addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
%\addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
%\addtolength{\abovedisplayskip}{-.5cm} % space before maths
%\addtolength{\belowdisplayskip}{-.5cm} % space after maths
%\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
%\setlength{\belowcaptionskip}{-.25cm}


% customize \paragraph spacing
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother



% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% supersense tag name
\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\uline{\textrm{#1}}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\sys}[1]{\mbox{\textbf{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments
\newcommand{\ilbl}[1]{\mbox{\textbf{\textsc{#1}}}} % internal label
\newcommand{\llbl}[1]{\mbox{\textsc{#1}}} % leaf label

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\tat}[0]{\textasciitilde}

\newcommand{\shortlong}[2]{#1} % short vs. long version of the paper
\newcommand{\confversion}[1]{#1}
\newcommand{\srsversion}[1]{}
%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\costversion}[1]{}
\newcommand{\shortversion}[1]{#1}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{} % ...if only there were more space...
\newcommand{\subversion}[1]{#1} % for the submission version only
\newcommand{\draftnotice}[1]{{\it\small #1}\\} % for the draft version only
%\newcommand{\subversion}[1]{}

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{SemCor}
\hyphenation{PennConverter}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}
\hyphenation{per-cept}
\hyphenation{per-cepts}
\title{\draftnotice{Title, author list not final. Page limit: 8 + 2 bib (will be granted more space in CR)}A Classifier for Communicative Functions of Definiteness}

\finalversion{\author{Archna Bhatia \ \ \  Chu-Cheng Lin \ \ \  Lori Levin \ \ \ \  Mandy Simons \\
\bf Fatima Talib Al-Raisi \ \ \  Laleh Roostapour \ \ \  Abhimanu Kumar  \\
\bf  Nathan Schneider \ \ \  Yulia Tsvetkov \ \ \  Jordan Bender \ \ \  Chris Dyer\\
Carnegie Mellon University\\
Pittsburgh, PA 15213}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
% Definiteness is a nonhomogeneous category across languages expressing various {\em communicative functions} 
% (types of semantic, pragmatic, and discourse--related information).   
% Languages differ in the grainsize and inventory of communicative functions  that are expressed 
% as well as the syntactic means for expressing them.  
% \citet{bhatia14} presented an annotation scheme for the communicative functions of definiteness 
% and released an annotated corpus in English and Hindi.   
Whether an NP is realized grammatically as definite or not 
depends on a variety of semantic, pragmatic, and discourse criteria, or {\em communicative functions}, 
the interaction of which varies from language to language.
We present a supervised classifier for English that uses lexical, morphological, and syntactic features 
to predict communicative functions of definiteness.   
The benefits of this work are twofold: linguistically, the classifier's features and weights 
model the {\em grammaticalization} of definiteness in English, 
not all of which are obvious.
% some of which is obvious (e.g., {\em the} and {\em a}) and some of which is not obvious \ab{(include an example from our findings)}. 
Computationally, it presents a framework to predict \textit{semantic and pragmatic} communicative functions of definiteness 
which, unlike lexical and morphosyntactic features, are preserved in translation.  
The classifier may therefore be useful for coreference resolution, MT
and other mono- and cross-lingual NLP tasks. 
\end{abstract}

\section{Introduction}

Languages display a vast range of variation with respect to the form and meaning of definiteness. 
For example, while languages like English make use of definite and indefinite articles 
to distinguish between the discourse status of various entities ({\it \textbf{the} car} vs.~{\it \textbf{a} car} vs.~{\it cars}), 
many other languages---including Czech, Indonesian, and Russian---do not have articles
% Archna: Hindi sort of has an indefinite article 
(although they do have demonstrative determiners).\footnote{Definite NPs, such as demonstratives, personal pronouns, and possessives are found in all languages.}  
% Some languages such as Hausa~\citep{lyons99} have different definite articles for noun phrases 
% that have been previously mentioned in contrast to those that are definite by virtue of the situation 
% (e.g., ``the podium'' at a conference).
Sometimes definiteness is marked with affixes or clitics, as in Arabic. Sometimes it is expressed with other constructions,
as in Chinese (a language without articles), where the existential construction is used to express indefinite subjects 
and the {\it ba}- construction to express definite direct objects \citep{chen04}. 

Aside from this variation in the form of (in)definite noun phrases (NPs) within and across languages, there is also variability 
in the semantic, pragmatic, and discourse--related functions expressed by (in)definites.   
We will refer to these as {\em communicative functions} of (in)definiteness.  
\Citet[pp. 6--7]{croft-03}, for instance, shows that the conditions under which an NP 
is marked as definite or indefinite (or not at all) 
are language-specific by contrasting English and French translations such as:
%In the CFD scheme, these should receive labels \llbl{nonuniq\_Hearer\_new\_spec}, \llbl{Other\_Nonreferential}, \llbl{Generic\_individualLevel}, and \llbl{Predicative\_equative\_role}, respectively.  The first is expressed the same (with a definite article) in English and French, whereas the other two are expressed differently in the two languages.     
%\eenumsentence{\small\label{ex:enfr}
% \item He went to \textbf{the bank}. (def.)\\
% 	  Il est all\'{e} \`{a} \textbf{la banque}. (def.)

\begin{small}
\enumsentence{\label{ex:care}He showed \textbf{extreme care}. (unmarked)\\
	  Il montra \textbf{un soin extr\^{e}me}. (indef.)}
\enumsentence{\label{ex:artichokes}I love \textbf{artichokes} and asparagus. (unmarked)\\
	  J'aime \textbf{les artichauts} et les asperges. (def.)}
\enumsentence{\label{ex:soldier}His brother became \textbf{a soldier}. (indef.)\\
	  Son fr\`{e}re est devenu \textbf{soldat}. (unmarked)}
\end{small}
A cross-linguistic classification of communicative functions  
should be able to characterize the aspects of meaning that account for the different patterns of definiteness marking 
exhibited in \cref{ex:care,ex:artichokes,ex:soldier}: e.g., that \cref{ex:artichokes} concerns a generic class of entities while 
\cref{ex:soldier} concerns a role filled by an individual. For more on communicative functions, see \cref{sec:scheme}.

\begin{figure*}[t]\small\centering
\begin{tabular}{@{}p{.51\textwidth}p{.46\textwidth}@{}}
\begin{itemize}
\item    \ilbl{Nonanaphora} $[-A,-B]$ \hfill {\bf\tiny 999}
  \begin{itemize}
  \item      \ilbl{Unique} $[+U]$ \hfill {\bf\tiny 287}
    \begin{itemize}
    \item        \ilbl{uniq\_Hearer\_old} $[-G,+O,+S]$ \hfill {\bf\tiny 251}
      \begin{itemize}
      \item          \llbl{uniq\_Physical\_copresence} $[+R]$ \hfill {\tiny 13}
      \item          \llbl{uniq\_Larger\_situation} $[+R]$ \hfill {\tiny 237}
      \item          \llbl{uniq\_predicative\_identity} $[+P]$ \hfill {\tiny 1}
      \end{itemize}
    \item        \llbl{uniq\_Hearer\_new} $[-O]$ \hfill {\tiny 36}
    \end{itemize}
   \item     \ilbl{Nonunique} $[-U]$ \hfill {\bf\tiny 581}
     \begin{itemize}
     \item       \ilbl{nonuniq\_Hearer\_old} $[+O]$ \hfill {\bf\tiny 169}
       \begin{itemize}
       \item         \llbl{nonuniq\_Physical\_copresence} $[-G,+R,+S]$ \hfill {\tiny 39}
       \item         \llbl{nonuniq\_Larger\_situation} $[-G,+R,+S]$ \hfill {\tiny 117}
       \item         \llbl{nonuniq\_predicative\_identity} $[+P]$ \hfill {\tiny 13}
       \end{itemize}
     \item       \llbl{nonuniq\_Hearer\_new\_spec} $[-G,-O,+R,+S]$ \hfill {\tiny 231}
     \item       \llbl{nonuniq\_nonspec} $[-G,-S]$ \hfill {\tiny 181}
     \end{itemize}
   \item \ilbl{Generic} $[+G,-R]$ \hfill {\bf\tiny 131}
     \begin{itemize}
	   \item      \llbl{Generic\_kindLevel} \hfill {\tiny 0}
	   \item      \llbl{Generic\_individualLevel} \hfill {\tiny 131}
     \end{itemize}
  \end{itemize}
\end{itemize} &
\begin{itemize}
\item \ilbl{Anaphora} $[+A]$ \hfill {\bf\tiny 1574}
  \begin{itemize}
  \item      \ilbl{Basic} $[+O,-B]$ \hfill {\bf\tiny 795}
    \begin{itemize}
    \item        \llbl{Same\_head} \hfill {\tiny 556}
    \item        \llbl{Different\_head} \hfill {\tiny 329}
    \end{itemize}
  \item  \ilbl{Extended} $[+B]$ \hfill {\bf\tiny 779}
    \begin{itemize}
    \item        \llbl{Bridging\_nominal} $[-G,+R,+S]$ \hfill {\tiny 43}
    \item        \llbl{Bridging\_event} $[+R,+S]$ \hfill {\tiny 10}
    \item        \llbl{Bridging\_restrictiveModifier} $[-G,+S]$ \hfill {\tiny 614}
    \item        \llbl{Bridging\_subtype\_instance} $[-G]$ \hfill {\tiny 0}
    \item        \llbl{Bridging\_OtherContext} $[+O]$ \hfill {\tiny 112}
    \end{itemize}
  \end{itemize}
\item \ilbl{Miscellaneous} $[-R]$ \hfill {\bf\tiny 732}
  \begin{itemize}
	\item \llbl{Pleonastic} $[-B,-P]$ \hfill {\tiny 53}
	\item \llbl{Quantified} \hfill {\tiny 248}
	\item \llbl{Predicative\_equative\_role} $[-B,+P]$ \hfill {\tiny 58}
	\item \llbl{Part\_of\_noncompositional\_mwe} \hfill {\tiny 100}
	\item \llbl{Measure\_Nonreferential} \hfill {\tiny 125}
	\item \llbl{Other\_Nonreferential} \hfill {\tiny 148}
  \end{itemize}
\end{itemize}
\end{tabular}
\begin{tabular}{r@{~~}>{\smaller}r@{~~}>{\smaller}r@{~~}>{\smaller}r|r@{~~}>{\smaller}r@{~~}>{\smaller}r@{~~}>{\smaller}r|r@{~~}>{\smaller}r@{~~}>{\smaller}r@{~~}>{\smaller}r|r@{~~}>{\smaller}r@{~~}>{\smaller}r@{~~}>{\smaller}r}
& \multicolumn{1}{@{}c}{$+$} & \multicolumn{1}{@{~~}c}{$-$} & \multicolumn{1}{@{~~}c}{$0$} && 
  \multicolumn{1}{@{~}c@{~}}{$+$} & \multicolumn{1}{@{~}c@{~~}}{$-$} & \multicolumn{1}{@{~~}c}{$0$} && 
  \multicolumn{1}{@{~}c@{~}}{$+$} & \multicolumn{1}{@{~}c@{~~}}{$-$} & \multicolumn{1}{@{~~}c}{$0$} && 
  \multicolumn{1}{@{~}c@{~}}{$+$} & \multicolumn{1}{@{~}c@{~~}}{$-$} & \multicolumn{1}{@{~~}c}{$0$} \\
\hline
\uline{A}naphoric & 1574 &  999 & 732 &    \uline{G}eneric &  131 & 1476 & 1698 & \uline{P}redicative &  72 &  53 & 3180 & \uline{S}pecific & 1305 & 181 & 1819 \\
\uline{B}ridging  &  779 & 1905 & 621 & Hearer-\uline{O}ld & 1327 &  267 & 1711 & \uline{R}eferential & 690 & 863 & 1752 & \uline{U}nique   &  287 & 581 & 2437 \\
\end{tabular}\finalversion{\nss{TODO: column headings aren't quite centered}}
\caption{CFD (Communicative Functions of Definiteness) annotation scheme, with frequencies in the corpus. Internal (non-leaf) labels are in bold;\costversion{\nss{}} these are not annotated or predicted.\nss{TODO: normalize capitalization}
$+$/$-$ values are shown for ternary attributes \uline{A}naphoric, \uline{B}ridging, \uline{G}eneric, Hearer-\uline{O}ld, \uline{P}redicative, \uline{R}eferential, \uline{S}pecific, and \uline{U}nique; 
these are inherited from supercategories, but otherwise default to $0$.
Thus, for example, the full attribute specification for \llbl{uniq\_Physical\_copresence} is $[-A,-B,-G,+O,0P,+R,+S,+U]$.
Counts for these attributes are shown in the table at bottom.}
\label{fig:hierarchy}
\end{figure*}

This paper describes a classifier that predicts communicative function labels for English NPs  
using lexical, morphological, and syntactic features.   
The contribution of our work is in both the output of the classifier and the model that it uses (features and weights).  %\ab{What about the Random forest classifier, we don’t mention that?}
The classifier predicts communicative function labels that capture aspects of discourse-newness, uniqueness, specificity, and so forth. %(see next section, e.g., whether the entities are 
%old or new to the discourse and to the hearer)
Such functions are usually preserved in translation, even when the grammatical mechanisms 
for expressing them are different. 
Indeed, previous work has noted that machine translation systems face problems 
while translating from one language to another when the languages use different grammatical strategies (see \cref{sec:related}).
% If the mapping between different forms and the information they encode is known
% in both the source language and the target language, this information can be leveraged in 
% improving machine translation across these languages.  
The communicative function labels also represent the discourse status of entities, 
making them relevant for entity tracking, knowledge base construction, and information extraction. 

The model is a form-meaning mapping, consisting of the syntactic, lexical, and morphological features 
and weights that are predictive of communicative functions.   
This in itself is linguistically significant in that it shows the grammatical mechanisms 
beyond the articles {\em the} and {\em a} that are used for expressing definiteness in English.   
%The form-meaning mapping for these languages can be used for machine translation applications. 
%\citet{tsvetkov13,stymne09} mention how translating from an article-language to an article-less language is problematic. 

To build our model, we leverage a cross-lingual definiteness annotation scheme (\cref{sec:scheme}) 
and annotated English corpus (\cref{sec:data}) from prior work \citep{bhatia14}.
Our classifier, \cref{sec:modeling}, is a supervised log-linear model akin to logistic regression,
with features that combine lexical and morphosyntactic information 
with prespecified groupings of the communicative function labels;
the evaluation measures (\cref{sec:eval}) include one that exploits these label groupings 
to award partial credit according to relatedness.
\nss{TODO: \cref{sec:exp} obtain good performance? discover interesting features? \cref{sec:eval,sec:exp,sec:related}}

%\ab{Here we discuss about why we should study definiteness- linguistically a hard problem, also it has applications in machine translation. Discuss about grammaticalization as a general problem (differences across languages), then grammaticalization of definiteness across languages, some examples to show differences across languages.}
%\nss{we review the annotation scheme in \cref{sec:scheme}; etc.}

\section{Annotation scheme}\label{sec:scheme}

The literature on definiteness describes functions such as 
uniqueness, familiarity, identifiability, anaphoricity, specificity, and 
referentiality \citep[\textit{inter alia}]{birner94,condoravdi92,evans77,evans80,gundel88,gundel93,heim90,kadmon87,kadmon90,lyons99,prince92,roberts03,russell05} as being related to definiteness.\footnote{The reductionist approaches to definiteness try to define it in terms of one or two of above mentioned communicative functions.   
For example, \citet{roberts03} proposes that the combination of uniqueness and a presupposition of familiarity 
underlie all definite descriptions.  However, possessive definite descriptions ({\it John's daughter}) 
and the weak definites ({\it the son of Queen Juliana of the Netherlands}) are neither unique nor necessarily 
familiar to the listener before they are spoken. In contrast to the reductionist approaches are approaches to grammaticalization~\citep{hopper-03} in which grammar develops over time in such a way that 
each grammatical construction has some prototypical communicative functions, 
but may also have many non-prototypical communicative functions. The scheme we are adopting assumes that there may be multiple functions to definiteness.} For this work, we have adopted a scheme that is based on a combination of these functions, the annotation scheme for Communicative Functions of Definiteness (CFD), as described in \citet{bhatia14}.
It is summarized in \cref{fig:hierarchy}.
  
It was developed by annotating texts in two languages (English and Hindi) for various genres keeping in mind the communicative functions that have been associated with definiteness previously. The hierarchical organization of CFD serves to reduce the number of decisions that an annotator needs to make
for speed and consistency.

It assigns a communicative function label to every NP except for first-person pronouns, second-person pronouns and relative pronouns.
The three main communicative functions in CFD are {\bf Anaphora} vs.~{\bf Nonanaphora} 
(whether the entity is old in the discourse or not), {\bf Hearer-old} vs.~{\bf Hearer-new}, 
and {\bf Unique} vs.~{\bf Nonunique} (annotated for {\bf Nonanaphoric} only in the current scheme).  
However there are a few twists.   
Entities that have not been mentioned are considered {\bf Anaphoric} (discourse-old) 
if they are evoked by a previously mentioned entity.   
For example, after mentioning a wedding, {\em the bride}, {\em the groom}, and {\em the cake} 
are considered to be {\bf Anaphoric}~\citep{clark77,poesio98}.  
Entities that are {\bf Non-Anaphoric} can be {\bf Hearer-old} if they are physically present 
in the speech situation ({\bf Physical-copresence}) or are not Physically-copresent but are identifiable by members of a community due to it being a part of the common knowledge retained by the community ({\bf Larger-situation},
e.g., spoken on the first day of a conference: ``I'm tired. The airplane was noisy.'').\footnote{\citet{komen-13} also proposed a hierarchy with similar leaf nodes, but different internal structure.}  

In addition to the three main communicative functions, we have annotations for 
generic, pleonastic, quantified, predicative, and non-referential NPs. For details on the scheme, see \citet{bhatia14}  

\Cref{fig:excerpt} is an excerpt from the ``Little Red Riding Hood'' annotated with the CFD scheme.


\begin{figure*}[t]\small
\textit{Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child.}
\\[5pt]
Once \glosst{she}{\llbl{Same\_head}} gave \glosst{her}{\llbl{Different\_head}} 
\glosst{a little riding hood of \glosst{red velvet}{\llbl{Other\_nonreferential}}}{$\underset{~}{\llbl{nonuniq\_Hearer\_new\_spec}}$}, 
which suited \glosst{her}{\llbl{Same\_head}} so well that \glosst{she}{\llbl{Same\_head}} 
would never wear \glosst{anything else}{\llbl{Quantified}}; so \glosst{she}{\llbl{Same\_head}} was always called 
`\glosst{Little Red Riding Hood}{\llbl{uniq\_Hearer\_new}}.'
\caption{An annotated sentence from ``Little Red Riding Hood.'' The previous sentence is shown for context.}
\label{fig:excerpt}
\end{figure*}




\section{Data}\label{sec:data}

We use the English definiteness corpus of \citet{bhatia14}, 
which consists of texts from multiple genres annotated with the scheme described in \cref{sec:scheme}. 
The 17~documents consist of prepared speeches (TED talks and a presidential address), 
published news articles, and fictional narratives. 
The TED data predominates (75\% of the corpus)\footnote{Note that the TED talks are from a large parallel corpus obtained from {\it http://www.ted.com/talks/}.}; 
the presidential speech represents about 16\%, fictional narratives 5\%, and news articles 4\%. 
All told, the corpus contains 13,860~words (868~sentences), with 3,422~NPs (the annotatable units). 
\citet{bhatia14} report high inter-annotator agreement, estimating Cohen's $\kappa = 0.89$ within the TED genre 
and $0.9$ for all genres.  


\section{Classification framework}\label{sec:modeling}

To model the relationship between the grammar of definiteness and its communicative functions in a data-driven fashion,
we work within the supervised framework of feature-rich discriminative classification, 
treating the functional categories from \cref{sec:scheme} as output labels $y$
and various lexical, morphological, and syntactic characteristics of the language as features of the input $x$.
Specifically, we learn a probabilistic log-linear model similar to multiclass logistic regression, 
but deviating in that
logistic regression treats each output label (response) as atomic, whereas 
we decompose each into \emph{attributes} based on their linguistic definitions, 
enabling commonalities between related labels to be recognized.
Each weight in the model corresponds to a feature that mediates between 
\emph{percepts} (characteristics of the input NP) and attributes (characteristics of the label).
\costversion{\nss{}the following ways:
\begin{itemize}
  \item Logistic regression treats each output label (response) as atomic; 
  we decompose each into \emph{attributes} based on their linguistic definitions, 
  enabling commonalities between related labels to be recognized.
  Each weight in the model corresponds to a feature that mediates between 
  \emph{percepts} (characteristics of the input NP) and attributes (characteristics of the label).
  \item Logistic regression assumes a prediction is either correct or incorrect.
  We incorporate a \emph{cost function} that gives partial credit during learning when a related label 
  is predicted, so the learned model will better match our evaluation measure.
  \item Logistic regression assumes the space of possible predictions matches 
  the space of labels observed in the training data; we allow more abstract labels to be predicted, 
  which can receive partial credit. The scoring scheme encourages the predictor to ``back off'' 
  to a coarser label if it is not sufficiently confident about a fine-grained label.
\end{itemize}
These decisions are\nss{}}
This is aimed at attaining better predictive accuracy 
as well as feature weights that better describe the form--function interactions we are interested in recovering.

Our setup is formalized below, where we discuss the mathematical models and linguistically motivated features.

\subsection{Model}

We experiment with two classification models: linear logistic regression and a non-linear tree-based model. 
Due to their consistency and interpretability, linear models are a valuable tool for quantifying and analyzing the effects of individual features; non-linear models, on the other hand, are powerful and often outperform logistic regression \citep{PerlichEtAl:2003}.

\subsubsection{Linear model}

At test time, we model the probability of semantic label $y$ 
conditional on an \nss{gold?} NP $x$ as follows:
\begin{equation}
p_{\boldsymbol{\theta}}(y | x) = \log{\frac{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y)}}{\sum_{y' \in \mathcal{Y}}\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y')}}}
\end{equation}
\cjd{Wouldn't it make more sense, given the story about percepts and label attributes, to use the form $\boldsymbol{\phi}(x)^{T} W \boldsymbol{\omega}(y)$? Then you could cast $W$ as a linear map from the percept vector space to the attribute vector space. This is a bit different than the usual linear regression story (of course), but it's closer to what you're actually doing. I think the links to the feature vector function of $x,y$ is a bit opaque and could be a footnote, but I'll leave it up to you.}
where $\boldsymbol{\theta} \in \mathbb{R}^d$ is a vector of parameters (feature weights), and 
$\mathbf{f}: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^d$ is the feature function over input--label pairs.
The feature function is defined as follows:
\begin{equation}
\mathbf{f}(x,y) = \boldsymbol{\phi}(x) \times \tilde{\boldsymbol{\omega}}(y)
\end{equation}
where the percept function $\boldsymbol{\phi}: \mathcal{X} \rightarrow \mathbb{R}^c$ 
produces a vector of real-valued characteristics of the input, and  
the attribute function\fta{define a and c to make it clear} $\tilde{\boldsymbol{\omega}}: \mathcal{Y} \rightarrow \{0,1\}^a$
encodes characteristics of each label.
There is a feature for every percept--attribute pairing: so
$d = c \cdot a$ and $f_{(i-1)a+j}(x,y) = \phi_i(x)\tilde{\omega}_j(y), 1 \leq i \leq c, 1 \leq j \leq a$.
The contents of the percept and attribute functions are detailed in \cref{sec:feats,sec:attrs}.

For prediction, having learned weights $\hat{\boldsymbol{\theta}}$ we choose the $y$ that maximizes this probability \cjd{maybe add: the Bayes-optimal decision rule for minimizing misclassification error}:
\begin{equation}
\hat{y} \leftarrow \arg\max_{y' \in \mathcal{Y}} p_{\hat{\boldsymbol{\theta}}}(y | x)
\end{equation}
Training optimizes $\hat{\boldsymbol{\theta}}$ so as to maximize a convex $L_1$-regularized
\costversion{\emph{softmax-margin} }learning objective\costversion{ \citep{gimpel}} over the training data $\mathcal{D}$:
\begin{equation}
\hat{\boldsymbol{\theta}} = \argmax_{\boldsymbol{\theta}} 
-\lambda ||\boldsymbol{\theta}||_1 
+ \sum_{\langle x,y \rangle\in\mathcal{D}} \log{\frac{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y)}}{\sum_{y \in \mathcal{Y}}\exp{\left(\boldsymbol{\theta}^{\top}\mathbf{f}(x,y')\costversion{ + \kappa\textit{cost}(y,y')}\right)}}}
\end{equation}\nss{TODO: fix regularizer}
\costversion{The \emph{cost function} allows us to penalize some errors more than others during training, 
taking into account the linguistic functions of the labels.
It is zero for the gold label and nonnegative for the others.\nss{intuition}

This framework gives us several ways to design a classifier appropriate to the task: 
the attributes, the space of labels $\mathcal{Y}$ to consider, 
and the cost function, and of course, the features themselves.}
With \costversion{$\kappa = 0$, $\mathcal{Y} = \{\textit{gold labels in training}\}$, 
and }$\tilde{\boldsymbol{\omega}}(y) = \textit{the identity of the label}$, 
this reduces to standard logistic regression.

\subsubsection{Non-linear model}
We employ a random forest classifier \citep{Breiman:2001}, an ensemble of tree-structured classifiers ${h(\mathbf{x}, N, \boldsymbol{\Theta})}$, 
where $\mathbf{x}$ is an input vector, $N$ is the number of classification trees, and $\boldsymbol{\Theta}$ is a set of $N$ independently generated random vectors. 
%The random vector for the $k$th tree---$\Theta_k$---is generated independently from the past $\Theta_1, \dots, \Theta_{k-1}$ vectors, but from the same distribution. 
\nss{they come from the same distribution---what is it?}
Each random vector governs the growth of its tree in the ensemble. 
After $N$ trees are generated, the most popular class is selected by voting; each tree casts a unit vote.
 
An important property of the random forests, in addition to being an effective tool in prediction, is their insusceptibility to overfitting. Rather than overfitting, as the number of trees increases, they produce a limiting value of the generalization error.\footnote{See Theorem 1.2 in \cite{Breiman:2001} for more details.} Thus, a held-out development set for hyperparameter tuning is not required; this alleviates the scarcity problem in setups with limited amount of data.

\subsection{Percepts}\label{sec:feats}

The characteristics of the input that are incorporated in the model, which we call \emph{percepts} 
to distinguish them from model features linking inputs to outputs,\footnote{See above.\fta{see section or subsection number?}} 
are intended to capture the aspects of English morphosyntax that may be relevant 
to the communicative functions of definiteness.

After preprocessing the text with a dependency parser and coreference resolver, 
we extract \sout{\bf the} several kinds of percepts for each NP.

\subsubsection{Basic}

\paragraph{Words of interest.} 
These are the \emph{head} within the NP, all of its \emph{dependents}, and its \emph{governor} (external to the NP). 
We are also interested in the \emph{attached verb}, which is the first verb one encounters when traversing the dependency path upward from the head. 
For each of these words, we have separate percepts capturing: the token, the part-of-speech (POS) tag, the lemma, 
the dependency relation,
and (for the head only) a binary indicator of plurality (determined from the POS tag).
As there may be multiple dependents, we have additional features specific to the first and the last one. 
Moreover, to better capture tense, aspect and modality, we collect the attached verb's \emph{auxiliaries}. 
We also make note of \emph{neg} if it is attached to the verb.
    
\paragraph{Structural.} 
The structural percepts are: the \emph{path length} from the head up to the root, and to the attached verb. 
We also have percepts for the number of dependents, and the number of dependency relations that link non-neighbors.
Integer values were binarized with thresholding.

\paragraph{Positional.} 
These percepts are the \emph{token length} of the NP, the NP's \emph{location} in the sentence (first or second half), and 
the \emph{attached verb's position} relative to the head (left or right). 
12~additional percept templates record the POS and lemma of the left and right neighbors of the head, governor, and attached verb.

\subsubsection{Contextual NPs}

When extracting features for a given NP (call it the ``target''), 
we also consider NPs in the following relationship with the target NP:
its \emph{immediate parent}, which is the smallest NP whose span fully subsumes that of the target; 
the \emph{immediate child}, which is the largest NP subsumed within the target;
the \emph{immediate precedent} and \emph{immediate successor} within the sentence; 
and the \emph{nearest preceding coreferent mention}.

For each of these related NPs, we include all of their basic percepts 
conjoined with the nature of the relation to the target.

\subsection{Attributes}\label{sec:attrs}

As noted above, though labels are organized into a tree hierarchy, 
there are actually several dimensions of commonality that suggest different groupings.
These attributes are encoded as ternary characteristics; 
for each label (including internal labels), every one of the 8 attributes  
is assigned a value of $+$, $-$, or $0$ (refer to \cref{fig:hierarchy}).
In light of sparse data, we design features\costversion{ and cost function} to exploit these similarities 
via the attribute vector function
\begin{equation*}
\boldsymbol{\omega}(y) = [y, A(y), B(y), G(y), O(y), P(y), R(y), S(y), U(y)]^{\top}
\end{equation*}
where $A: \costversion{\mathcal{L} \cup \mathcal{I}\nss{}}\mathcal{Y} \rightarrow \{+, -, 0\}$ returns the value for Anaphoric, 
$B(y)$ for Bridging, etc.\fta{explain the three possible values: positive negative or 0} The identity of the label is also included in the vector so that 
different labels are always recognized as different by the attribute function.
The categorical components of this vector are then binarized to form $\tilde{\boldsymbol{\omega}}(y)$; 
however, instead of a binary component that fires for the $0$ value of each ternary attribute, 
there is a component that fires for \emph{any} value of the attribute---a sort of bias term.
The weights assigned to features incorporating $+$ or $-$ attribute values, then, 
are easily interpreted as deviations relative to the bias.

\costversion{\subsection{Cost and label space}\label{sec:cost}

The definiteness function hierarchy presented in \cref{fig:hierarchy} 
consists of 24~\emph{leaf labels}, which will be denoted $\mathcal{L}$, and 
10~more abstract \emph{intermediate labels}, denoted $\mathcal{I}$.
All of the gold labels in the training data are from $\mathcal{L}$, 
but we give our model the option to predict more abstract labels 
to receive partial credit.
We will therefore use $\mathcal{Y} = \mathcal{L} \cup \mathcal{I}$.

The relatedness of label pairs depends on the number of values that differ between the two original attribute vectors: 
let $\Delta(y,y') = |\boldsymbol{\omega}(y) \ominus \boldsymbol{\omega}(y')|$.\footnote{By the symmetric difference of two attribute vectors, we mean the subset of components that lack a matching (categorical) value.} 
For a gold leaf label $\ell$ and an internal label $\iota$ that subsumes it in the hierarchy 
(i.e., $\iota$ is an ancestor of $\ell$), let $\delta(\ell,\iota) =$ the distance in the hierarchy 
between $\ell$ and $\iota$.\footnote{There is no need to define $\delta(\iota, \cdot)$, as the training set does not contain intermediate labels.}
Now define $\textit{cost}(y,y') = \Delta(y,y') - 0.5^{\delta(y,y')}$: 
this assigns lower (better) cost to an internal label than to an incorrect leaf label with an equivalent attribute distance.}

\section{Evaluation}\label{sec:eval}

The following measures will be used to evaluate our predictor against the gold standard 
for the held-out evaluation (dev or test) set $\mathcal{E}$:
\begin{itemize}
  \item \textbf{Exact match:} This accuracy measure gives credit only where the predicted and gold labels 
  are identical.\costversion{\nss{} When the model is allowed to predict internal labels, we will report 
  overall precision and recall of leaf labels. Otherwise, we report accuracy.}
  \item \textbf{By leaf label:} We also compute precision and recall of each leaf label 
  to determine which categories are reliably predicted.
  \item \textbf{Soft match:} This accuracy measure gives partial credit where the predicted and gold labels 
  are related. It is computed as the proportion of attributes whose (categorical) values match: 
  $|\boldsymbol{\omega}(y) \cap \boldsymbol{\omega}(y')|/9$.\cjd{Should these be differentially weighted based on their height in the hierarchy?}
  \item \textbf{Perplexity:} Perplexity is a value $\rho \in [1,\infty)$ that quantifies how ``surprised'', on average, our model is by the gold labels 
  in the test set; the greater the probability mass assigned to the true labels, 
  the lower the score. 
  It is computed as $\rho = 2^{-\left(\sum_{\langle x, y \rangle \in \mathcal{E}} \log_2 p_{\hat{\boldsymbol{\theta}}}(y \mid x)\right) / |\mathcal{E}|}$. Intuitively a perplexity of $\rho$ means the amount of surprisal in a $\rho$-way uniform random draw.
\end{itemize}

\section{Experiments}\label{sec:exp}

\subsection{Experimental Setup}

The annotated corpus of \citet{bhatia14} (\cref{sec:data}) contains 17~documents 
in 3~genres: 13~prepared speeches (mostly TED talks), 2~newspaper articles, 
and 2~fictional narratives. We arbitrarily choose some documents to hold out from each genre; 
the resulting test set consists of 2~TED talks (``Alisa\_News'', ``RobertHammond\_park''), 
1~newspaper article (``crime1\_iPad\_E''), and 1~narrative (``Little Red Riding Hood'').
The test set then contains 19,28~tokens (111~sentences), in which there are 511~annotated NPs; 
while the training set contains 2,911~NPs among 11,932~tokens (757~sentences).
Gold NP boundaries are assumed throughout our experiments.

We use an in-house implementation of supervised learning with $L_1$-regularized AdaGrad \citep{adagrad}. \ab{Now we mention $L_2$-regularization as well as or instead of $L_1$.}
Hyperparameters are tuned on a dev set formed by holding out every tenth instance from the training set 
(test set experiments use the full training set).\nss{early stopping? what exactly is tuned? optimize soft match acc?} 
Automatic dependency parses and coreference information were obtained with 
the parser and coreference resolution system in Stanford CoreNLP v.~3.3.0 \citep{socher-13,recasens-13}
for use in features (\cref{sec:feats}).

\subsection{Results}

\begin{table*}\small\centering
\begin{tabular}{@{}lrrrrrr@{}}
\multicolumn{1}{c}{\bf Condition} & \multicolumn{1}{c}{$|\boldsymbol{\theta}|$} & \multicolumn{1}{c}{$||\boldsymbol{\theta}||_0$} & \multicolumn{1}{c}{$\lambda$} 
& \multicolumn{1}{c}{\bf Exact Match Acc.} & \multicolumn{1}{c}{\bf Soft Match Acc.} 
& \multicolumn{1}{c}{\bf Perplexity} \\
\midrule
Majority baseline & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} & \\
Log-linear classifier, no grouping by attributes & & \\
Full log-linear classifier & & \\
\end{tabular}
\caption{Classifier versus baselines, as measured on the test set. 
The first three columns of numbers report the number of parameters (feature weights), the number of nonzero parameters, 
and the tuned regularization hyperparameter, respectively.}
\label{tbl:results}
\end{table*}

\nss{English: ±cost function, ±non-identity attributes, ±predicting intermediate labels}

\nss{maybe: which attribute groupings produce the best classifier, if we want to force a hierarchy}

\nss{feature/attribute ablations}

\ab{a brief discussion of error analysis, feature weights-function mapping for possible grammaticalization??}


\section{Related Work}\label{sec:related}

Understanding definiteness is beneficial in applications such as coreference processing, information extraction, machine translation. For example, the anaphoric relations between a definite description and its antecedent could be realized differently depending on whether it is basic anaphora or extended anaphora and yet differently from non anaphoric uses of descriptions (NPs). A knowledge of these differences can aid coreference resolution systems in determining which entities may be linked together or in recognizing descriptions which are discourse-new depending on the grammaticalization they appear with. 

Similarly an understanding of definiteness in different languages can help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information , see \cite{stymne09}, \cite{tsvetkov13}. Previous work on machine translation (PBSMT) has attempted to deal with this in terms of either preprocessing the source language to make it look more like the target language, e.g. \cite{collins05, habash07, niesen00, stymne09}; or post processing the machine translation output to match the target language, e.g. \cite{popovic06}. Attempts have also been made to use syntax on either the source or the target side or on both sides to get the syntactic differences between languages \cite{liu06, yamada02, zhang07}. Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output \citep{knight1994automated}, text generation \citep{DBLP:conf/aaai/Elhadad93,minnen2000memory}, and identification and correction of ESL errors \citep{aehan2006detecting,de2008classifier,gamon2008,rozovskaya2010training}. More recently, \citet{tsvetkov13} trained a classifier to predict where English articles might plausibly be added or removed in a phrase, 
and used this classifier to improve the quality of statistical machine translation. 

While definiteness morpheme prediction has been thoroughly studied in computational linguistics, studies on additional, more complex aspects of definiteness are limited.  
\citet{reiter10}  exploit linguistically-motivated features in a supervised approach to 
distinguish between generic and specific NPs.\finalversion{\yt{what else?}}
To the best of our knowledge, no studies have been conducted 
on automatic prediction of semantic and pragmatic communicative functions of definiteness. An understanding of such communicative functions in a language can come handy, e.g., in cases of syncretism on the source and/or the target side language, where the observable parts of grammar alone cannot guide machine translation systems. 

%\citet{bresnan07} have used logistic regression to predict syntactic structures related to dative alternation based on the semantic mappings to the structures. %\nss{mention Bresnan's work on predicting syntactic alternations with logistic regression 
%(here we want to predict the hidden information so that the classifier is useful for applications\!).}


Our work is also related to research in linguistics on the modeling of syntactic constructions such as dative shift and the expression of possession with "of" or "'s" in approach.  \citet{bresnan07} used semantic features to predict syntactic constructions.   Although we are doing the opposite (using syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following~\citet{Hopper&Traugott, Traugott} we observe that grammaticalization is accompanied by {\em function drift}, resulting in multiple communicative functions for each grammatical construction. \ab{Hence, like \citet{bresnan07}, in this work, we used logistic regression models to explore the syntax-semantic mapping related to definiteness since such models are robust to deal with skewed data as well and hence are able to model probability without assuming a particular distribution of data.???!}



\nss{what about coref? discourse models?} 


\section{Conclusion}\label{sec:conclusion}

In this paper we presented a model that captures the relationship between the grammar of definiteness and its communicative functions in English to a good accuracy\ab{??! English is an article language, where articles are a clear indicator of some of these functions. }. In future work we will build such models for languages that do not have articles such as Hindi, Russian, and Chinese to identify grammaticalizations of definiteness in these languages as well. This information can, in turn, be used for various MT and NLP tasks.




\bibliographystyle{aclnat}
% you bib file should really go here
\setlength{\bibsep}{10pt}
{\fontsize{10}{12.25}\selectfont
\bibliography{definiteness}}


\end{document}


\section{Discussion: Error analysis, comparison across languages}\label{sec:discussion}

\ab{confusion matrix - Hindi vs.~Eng (for common annotations)- semantic annotations
differences in annotations- why- different grammatical constructions, e.g. NP in 1 language but predicate in another etc.
what do similarities tell us about grammaticalization, what do differences tell.
discuss- how this analysis can help in MT}

%A total of 6 annotators contributed to the annotations. Two of the annotators are linguists while others are computer scientists working on languages and an undergraduate student of Linguistics. Two annotators annotated most of the data, the inter annotator agreement between them was very high with Cohen's Kappa score of 0.94 within the TED corpus and 0.91 for combined genres.  However the other 4 annotators also annotated some of the data, but one of the two annotators with most experience took these data as inputs and revised annotations according to their annotations. This two-step process of annotation was taken to increase the amount of data within a short period of time and also to achieve consistency within annotations. All annotators were trained using the annotation guidelines and example annotated texts. Then the annotators were asked to annotate some data, these annotations were discussed to reach at consensus. The annotations used for discussions to reach at consensus were not included while calculating the inter annotator agreement.

\ab{description of data, IAA, other statistics- n(annotations), n(annotations/ category)
classifiers for English- baseline, accuracy etc, confusion matrix- classifier predictions and gold standard
may be a brief discussion of feature ablation study-  which features are most helpful at least in these languages corresponding to individual categories (a clue to grammaticalization)
Principle component analysis, clustering of features}

\fta{not totally clear how this relates to learning the communicative functions of definiteness, 
either elaborate or remove the part that start with In contrast to the reductionist...until prototypical functions. 
The part that follows in the previous version is relevant: Grammaticalization varies a lot across ...}

\nss{Hindi?}
