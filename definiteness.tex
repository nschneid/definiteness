\PassOptionsToPackage{usenames}{color}
\documentclass[11pt,letterpaper]{article}
\usepackage{comment}
\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{colingstyle/coling2014}

\usepackage[linkcolor=blue]{hyperref}

\usepackage[round]{natbib}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})}

%\usepackage{times}
%\usepackage{latexsym}


\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage[procnames]{listings}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

\usepackage{mathptmx}	% txfonts
\usepackage[scaled=0.8]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}





% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}


\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
\usepackage{tikz}
%\usepackage{tree-dvips}
\usetikzlibrary{arrows,positioning,calc} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{verbatim}
\usepackage{ulem}


% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
\makeatletter
\lst@AddToHook{EveryPar}{%
  \label{lst:\thelstnumber}% make a label for each line number except the first (assumes only one listing in the document)
}
\makeatother
\lstset{
% basicstyle=\rmshape,
  numbers=left,
  numberstyle=\tt\color{gray},
  firstnumber=2,
  stepnumber=5,
  xleftmargin=3em,
  language=Python,
  upquote=true,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=1,
  stringstyle=\color{mdgreen},
  commentstyle=\itshape\color{lavender},
  basicstyle=\small\smaller\ttfamily,
  morekeywords={lambda,with,as,assert},
  keywordstyle=\bfseries\color{magenta},
  procnamekeys={def},
  procnamestyle=\bfseries\color{orange},
  aboveskip=0.5cm,
  belowskip=0.5cm
}
\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\abmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{A}}_{\textsc{B}}}}}}
\newcommand{\ytmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{Y}}_{\textsc{T}}}}}}
\newcommand{\ftamarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{FT}}_{\textsc{A}}}}}}
\newcommand{\lslmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{LS}}_{\textsc{L}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\ab}[1]{\arkcomment{\abmarker}{#1}{red}}
\newcommand{\yt}[1]{\arkcomment{\ytmarker}{#1}{blue}}
\newcommand{\fta}[1]{\arkcomment{\ftamarker}{#1}{orange}}
\newcommand{\lsl}[1]{\arkcomment{\lslmarker}{#1}{purple}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\usepackage{cleveref}

% use \S for all references to all kinds of sections, and \P to paragraphs
% (sadly, we cannot use the simpler \crefname{} macro because it would insert a space after the symbol)
\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
\crefmultiformat{part}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{chapter}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{section}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsubsection}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{part}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{chapter}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{section}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsubsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{paragraph}{\mbox{\P\P#3#1#4--#5#2#6}}
\crefrangeformat{subparagraph}{\mbox{\P\P#3#1#4--#5#2#6}}
% for \label[appsec]{...}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{enums,enumsi}{example}{examples}
\Crefname{enums,enumsi}{Example}{Examples}
\crefformat{enums}{(#2#1#3)}
\crefformat{enumsi}{(#2#1#3)}
\crefrangeformat{enums}{\mbox{(#3#1#4--#5#2#6)}}
\crefrangeformat{enumsi}{\mbox{(#3#1#4--#5#2#6)}}

\ifx\creflastconjunction\undefined%
\newcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\else%
\renewcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\fi%

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
\newcommand{\fnref}[1]{\autoref{#1}} % don't use \cref{} due to bug in (now out-of-date) cleveref package w.r.t. footnotes


% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
%\addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
%\addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
%\addtolength{\abovedisplayskip}{-.5cm} % space before maths
%\addtolength{\belowdisplayskip}{-.5cm} % space after maths
%\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
%\setlength{\belowcaptionskip}{-.25cm}


% customize \paragraph spacing
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother



% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% supersense tag name
\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\uline{\textrm{#1}}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\sys}[1]{\mbox{\textbf{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments
\newcommand{\ilbl}[1]{\mbox{\textbf{\textsc{#1}}}} % internal label
\newcommand{\llbl}[1]{\mbox{\textsc{#1}}} % leaf label

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\tat}[0]{\textasciitilde}

\newcommand{\shortlong}[2]{#1} % short vs. long version of the paper
\newcommand{\confversion}[1]{#1}
\newcommand{\srsversion}[1]{}
%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\costversion}[1]{}
\newcommand{\shortversion}[1]{#1}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{} % ...if only there were more space...
\newcommand{\subversion}[1]{#1} % for the submission version only
\newcommand{\draftnotice}[1]{{\it\small #1}\\} % for the draft version only
%\newcommand{\draftnotice}[1]{}
%\newcommand{\subversion}[1]{}

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{SemCor}
\hyphenation{PennConverter}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}
\hyphenation{per-cept}
\hyphenation{per-cepts}
\hyphenation{post-edit-ing}
\title{Automatic Classification of Communicative Functions of Definiteness}

\author{Archna Bhatia$^\ast$ \ \ \ Chu-Cheng Lin$^\ast$ \ \ \ Nathan Schneider$^\ast$ \ \ \ Yulia Tsvetkov$^\ast$ & \bf Fatima Talib Al-Raisi$^\ast$ \ \ \  Laleh Roostapour$^\ast$ \ \ \ Jordan Bender$^\dagger$  \ \ \ Abhimanu Kumar$^\ast$  & \bf Lori Levin$^\ast$ \ \ \ Mandy Simons$^\ast$ \ \ \  Chris Dyer$^\ast$\\
$^\ast$Carnegie Mellon University\\
Pittsburgh, PA 15213\\
$^\dagger$University of Pittsburgh\\
Pittsburgh, PA 15260}

\date{}

\begin{document}
\maketitle
\begin{abstract}
% Definiteness is a nonhomogeneous category across languages expressing various {\em communicative functions} 
% (types of semantic, pragmatic, and discourse--related information).   
% Languages differ in the grainsize and inventory of communicative functions  that are expressed 
% as well as the syntactic means for expressing them.  
% \citet{bhatia14} presented an annotation scheme for the communicative functions of definiteness 
% and released an annotated corpus in English and Hindi.   
Definiteness expresses a constellation of semantic, pragmatic, and discourse properties---the communicative function---of an NP. We present a supervised classifier for English NPs that uses lexical, morphological, and syntactic features to predict an NP's communicative function in terms of a language-universal classification scheme. Our classifiers establish strong baselines for future work in this neglected area of computational semantic analysis. In addition, analysis of the features and learned parameters 
in the model provides insight into the grammaticalization of definiteness in English, 
not all of which is obvious a priori.
% some of which is obvious (e.g., {\em the} and {\em a}) and some of which is not obvious \ab{(include an example from our findings)}. 
\end{abstract}

\section{Introduction}

Definiteness has been taken to associate morphosyntax of Noun Phrases (NPs) with aspects of their meaning, e.g. definite markers have prototypically been associated with identifiability
(whether a referent for the NP can be identified by the discourse participants or not) \citep{lyons99}, together with other aspects such as inclusiveness (where the reference is made to the totality of objects or mass in the context) \citep{hawkins78}. 
While some morphosyntactic forms of definiteness are employed by all languages---namely, 
demonstratives, personal pronouns, and possessives---languages display a vast range of variation with respect to 
the form and meaning of definiteness. 
For example, while languages like English make use of definite and indefinite articles 
to distinguish between the discourse status of various entities ({\it \textbf{the} car} vs.~{\it \textbf{a} car} vs.~{\it cars}), 
many other languages---including Czech, Indonesian, and Russian---do not have articles
% Archna: Hindi sort of has an indefinite article 
(although they do have demonstrative determiners).  
% Some languages such as Hausa~\citep{lyons99} have different definite articles for noun phrases 
% that have been previously mentioned in contrast to those that are definite by virtue of the situation 
% (e.g., ``the podium'' at a conference).
Sometimes definiteness is marked with affixes or clitics, as in Arabic. Sometimes it is expressed with other constructions,
as in Chinese (a language without articles), where the existential construction can be used to express indefinite subjects 
and the {\it ba}- construction can be used to express definite direct objects \citep{chen04}. 

Aside from this variation in the form of (in)definite NPs within and across languages, there is also variability 
in the mapping between semantic, pragmatic, and discourse %--related 
functions of NPs and the (in)definites expressing these functions.   
We refer to these as {\em communicative functions} of definiteness, following \cite{bhatia14}.  
\Citet[pp. 6--7]{croft-03} shows that even when two languages have access to the same morphosyntactic forms of definiteness, the conditions under which an NP 
is marked as definite or indefinite (or not at all) 
are language-specific. He illustrates this by contrasting English and French translations (both languages use definite as well as indefinite articles) such as:
%In the CFD scheme, these should receive labels \llbl{nonuniq\_Hearer\_new\_spec}, \llbl{Other\_Nonreferential}, \llbl{Generic\_individualLevel}, and \llbl{Predicative\_equative\_role}, respectively.  The first is expressed the same (with a definite article) in English and French, whereas the other two are expressed differently in the two languages.     
%\eenumsentence{\small\label{ex:enfr}
% \item He went to \textbf{the bank}. (def.)\\
% 	  Il est all\'{e} \`{a} \textbf{la banque}. (def.)

\begin{small}
\enumsentence{\label{ex:care}He showed \textbf{extreme care}. (unmarked)\\
	  Il montra \textbf{\underline{un} soin extr\^{e}me}. (indef.)}
\enumsentence{\label{ex:artichokes}I love \textbf{artichokes} and asparagus. (unmarked)\\
	  J'aime \textbf{\underline{les} artichauts} et les asperges. (def.)}
\enumsentence{\label{ex:soldier}His brother became \textbf{\underline{a} soldier}. (indef.)\\
	  Son fr\`{e}re est devenu \textbf{soldat}. (unmarked)}
\end{small}
A cross-linguistic classification of communicative functions  
should be able to characterize the aspects of meaning that account for the different patterns of definiteness marking 
exhibited in \cref{ex:care,ex:artichokes,ex:soldier}: e.g., that \cref{ex:artichokes} concerns a generic class of entities while 
\cref{ex:soldier} concerns a role filled by an individual. For more on communicative functions, see \cref{sec:scheme}.

\begin{figure*}[t]\small\centering
\begin{tabular}{@{}p{.51\textwidth}p{.46\textwidth}@{}}
\begin{itemize}
\item    \ilbl{Nonanaphora} {\bf\scriptsize $[-A,-B]$} \hfill {\bf\tiny 999}
  \begin{itemize}
  \item      \ilbl{Unique} {\bf\scriptsize $[+U]$} \hfill {\bf\tiny 287}
    \begin{itemize}
    \item        \ilbl{Unique\_Hearer\_Old} {\bf\scriptsize $[+F,-G,+S]$} \hfill {\bf\tiny 251}
      \begin{itemize}
      \item          \llbl{Unique\_Physical\_Copresence} {\bf\scriptsize $[+R]$} \hfill {\tiny 13}
      \item          \llbl{Unique\_Larger\_Situation} {\bf\scriptsize $[+R]$} \hfill {\tiny 237}
      \item          \llbl{Unique\_Predicative\_Identity} {\bf\scriptsize $[+P]$} \hfill {\tiny 1}
      \end{itemize}
    \item        \llbl{Unique\_Hearer\_New} {\bf\scriptsize $[-F]$} \hfill {\tiny 36}
    \end{itemize}
   \item     \ilbl{Nonunique} {\bf\scriptsize $[-U]$} \hfill {\bf\tiny 581}
     \begin{itemize}
     \item       \ilbl{Nonunique\_Hearer\_Old} {\bf\scriptsize $[+F]$} \hfill {\bf\tiny 169}
       \begin{itemize}
       \item         \llbl{Nonunique\_Physical\_Copresence} {\bf\scriptsize $[-G,+R,+S]$} \hfill {\tiny 39}
       \item         \llbl{Nonunique\_Larger\_Situation} {\bf\scriptsize $[-G,+R,+S]$} \hfill {\tiny 117}
       \item         \llbl{Nonunique\_Predicative\_Identity} {\bf\scriptsize $[+P]$} \hfill {\tiny 13}
       \end{itemize}
     \item       \llbl{Nonunique\_Hearer\_New\_Spec} {\bf\scriptsize $[-F,-G,+R,+S]$} \hfill {\tiny 231}
     \item       \llbl{Nonunique\_Nonspec} {\bf\scriptsize $[-G,-S]$} \hfill {\tiny 181}
     \end{itemize}
   \item \ilbl{Generic} {\bf\scriptsize $[+G,-R]$} \hfill {\bf\tiny 131}
     \begin{itemize}
	   \item      \llbl{Generic\_Kind\_Level} \hfill {\tiny 0}
	   \item      \llbl{Generic\_Individual\_Level} \hfill {\tiny 131}
     \end{itemize}
  \end{itemize}
\end{itemize} &
\begin{itemize}
\item \ilbl{Anaphora} {\bf\scriptsize $[+A]$} \hfill {\bf\tiny 1574}
  \begin{itemize}
  \item      \ilbl{Basic\_Anaphora} {\bf\scriptsize $[-B,+F]$} \hfill {\bf\tiny 795}
    \begin{itemize}
    \item        \llbl{Same\_Head} \hfill {\tiny 556}
    \item        \llbl{Different\_Head} \hfill {\tiny 329}
    \end{itemize}
  \item  \ilbl{Extended\_Anaphora} {\bf\scriptsize $[+B]$} \hfill {\bf\tiny 779}
    \begin{itemize}
    \item        \llbl{Bridging\_Nominal} {\bf\scriptsize $[-G,+R,+S]$} \hfill {\tiny 43}
    \item        \llbl{Bridging\_Event} {\bf\scriptsize $[+R,+S]$} \hfill {\tiny 10}
    \item        \llbl{Bridging\_Restrictive\_Modifier} {\bf\scriptsize $[-G,+S]$} \hfill {\tiny 614}
    \item        \llbl{Bridging\_Subtype\_Instance} {\bf\scriptsize $[-G]$} \hfill {\tiny 0}
    \item        \llbl{Bridging\_Other\_Context} {\bf\scriptsize $[+F]$} \hfill {\tiny 112}
    \end{itemize}
  \end{itemize}
\item \ilbl{Miscellaneous} {\bf\scriptsize $[-R]$} \hfill {\bf\tiny 732}
  \begin{itemize}
	\item \llbl{Pleonastic} {\bf\scriptsize $[-B,-P]$} \hfill {\tiny 53}
	\item \llbl{Quantified} \hfill {\tiny 248}
	\item \llbl{Predicative\_Equative\_Role} {\bf\scriptsize $[-B,+P]$} \hfill {\tiny 58}
	\item \llbl{Part\_of\_Noncompositional\_mwe} \hfill {\tiny 100}
	\item \llbl{Measure\_Nonreferential} \hfill {\tiny 125}
	\item \llbl{Other\_Nonreferential} \hfill {\tiny 148}
  \end{itemize}
\end{itemize}
\end{tabular}
\begin{tabular}{r@{~~}>{\smaller}r@{~~}>{\smaller}r@{~~}>{\smaller}r|r@{~~}>{\smaller}r@{~~}>{\smaller}r@{~~}>{\smaller}r|r@{~~}>{\smaller}r@{~~}>{\smaller}r@{~~}>{\smaller}r|r@{~~}>{\smaller}r@{~~}>{\smaller}r@{~~}>{\smaller}r}
& \multicolumn{1}{@{}c}{$+$} & \multicolumn{1}{@{~~}c}{$-$} & \multicolumn{1}{@{~~}c}{$0$} && 
  \multicolumn{1}{@{~}c@{~}}{$+$} & \multicolumn{1}{@{~}c@{~~}}{$-$} & \multicolumn{1}{@{~~}c}{$0$} && 
  \multicolumn{1}{@{~}c@{~}}{$+$} & \multicolumn{1}{@{~}c@{~~}}{$-$} & \multicolumn{1}{@{~~}c}{$0$} && 
  \multicolumn{1}{@{~}c@{~}}{$+$} & \multicolumn{1}{@{~}c@{~~}}{$-$} & \multicolumn{1}{@{~~}c}{$0$} \\
\hline
\uline{A}naphoric & 1574 &  999 & 732 &    \uline{G}eneric &  131 & 1476 & 1698 & \uline{P}redicative &  72 &  53 & 3180 & \uline{S}pecific & 1305 & 181 & 1819 \\
\uline{B}ridging  &  779 & 1905 & 621 & \uline{F}amiliar & 1327 &  267 & 1711 & \uline{R}eferential & 690 & 863 & 1752 & \uline{U}nique   &  287 & 581 & 2437 \\
\end{tabular}\finalversion{\nss{TODO: column headings aren't quite centered}}
\caption{CFD (Communicative Functions of Definiteness) annotation scheme, with frequencies in the corpus. Internal (non-leaf) labels are in bold;\costversion{\nss{}} these are not annotated or predicted.\finalversion{\nss{TODO: normalize capitalization}}
$+$/$-$ values are shown for ternary attributes \uline{A}naphoric, \uline{B}ridging, \uline{F}amiliar, \uline{G}eneric, \uline{P}redicative, \uline{R}eferential, \uline{S}pecific, and \uline{U}nique; 
these are inherited from supercategories, but otherwise default to $0$.
Thus, for example, the full attribute specification for \llbl{Unique\_Physical\_Copresence} is $[-A,-B,+F,-G,0P,+R,+S,+U]$.
Counts for these attributes are shown in the table at bottom.}
\label{fig:hierarchy}
\end{figure*}

This paper develops supervised classifiers to predict communicative function labels for English NPs  
using lexical, morphological, and syntactic features.   
The contribution of our work is in both the output of the classifiers and the models themselves (features and weights).  %\ab{What about the Random forest classifier, we don’t mention that?}
Each classifier predicts communicative function labels that capture aspects of discourse-newness, uniqueness, specificity, and so forth. %(see next section, e.g., whether the entities are 
%old or new to the discourse and to the hearer)
Such functions are useful in a variety of language processing applications. For example, they should usually be preserved in translation, even when the grammatical mechanisms 
for expressing them are different. 
%Indeed, previous work has noted that machine translation systems face problems 
%while translating from one language to another when the languages use different grammatical strategies (see \cref{sec:related}).
% If the mapping between different forms and the information they encode is known
% in both the source language and the target language, this information can be leveraged in 
% improving machine translation across these languages.  
The communicative function labels also represent the discourse status of entities, 
making them relevant for entity tracking, knowledge base construction, and information extraction. 

Our log-linear model is a form-meaning mapping that relates syntactic, lexical, and morphological features 
to properties of communicative functions. The learned weights of this model can, e.g., generate plausible hypotheses regarding the form-meaning relationship which can then be tested rigorously through controlled experiments. 
This hypothesis generation is linguistically significant as it indicates new grammatical mechanisms 
beyond the obvious {\em a} and {\em the} articles that are used for expressing definiteness in English. 
%The form-meaning mapping for these languages can be used for machine translation applications. 
%\citet{tsvetkov13,stymne09} mention how translating from an article-language to an article-less language is problematic. 


To build our models, we leverage a cross-lingual definiteness annotation scheme (\cref{sec:scheme}) 
and annotated English corpus (\cref{sec:data}) developed in prior work \citep{bhatia14}.
The classifiers, \cref{sec:modeling}, are supervised models
with features that combine lexical and morphosyntactic information 
and the prespecified attributes or groupings of the communicative function labels (such as Anaphoric, Bridging, Specific in \cref{fig:hierarchy}) to predict leaf labels (the non-bold faced labels in \cref{fig:hierarchy}); %\nss{this is a bit technical and fig. 1 hasn't been introduced yet. can it be simplified?}%\ab{couldn’t get to it}
the evaluation measures (\cref{sec:eval}) include one that exploits these label groupings 
to award partial credit according to relatedness.
\Cref{sec:exp} presents experiments comparing several models and discussing their strengths and weaknesses;
computational work and applications related to definiteness are addressed in \cref{sec:related}.

%\ab{Here we discuss about why we should study definiteness- linguistically a hard problem, also it has applications in machine translation. Discuss about grammaticalization as a general problem (differences across languages), then grammaticalization of definiteness across languages, some examples to show differences across languages.}
%\nss{we review the annotation scheme in \cref{sec:scheme}; etc.}

\section{Annotation scheme}\label{sec:scheme}

The literature on definiteness describes functions such as 
uniqueness, familiarity, identifiability, anaphoricity, specificity, and 
referentiality \citep[\textit{inter alia}]{birner94,condoravdi92,evans77,evans80,gundel88,gundel93,heim90,kadmon87,kadmon90,lyons99,prince92,roberts03,russell05} as being related to definiteness. Reductionist approaches to definiteness try to define it in terms of one or two of the aforementioned communicative functions.   
For example, \citet{roberts03} proposes that the combination of uniqueness and a presupposition of familiarity 
underlie all definite descriptions.  However, possessive definite descriptions ({\it John's daughter}) 
and the weak definites ({\it the son of Queen Juliana of the Netherlands}) are neither unique nor necessarily 
familiar to the listener before they are spoken. In contrast to the reductionist approaches are approaches to grammaticalization~\citep{hopper-03} in which grammar develops over time in such a way that 
each grammatical construction has some prototypical communicative functions, 
but may also have many non-prototypical communicative functions. The scheme we are adopting for this work---the annotation scheme for Communicative Functions of Definiteness (CFD) as described in \citet{bhatia14}---assumes that there may be multiple functions to definiteness. CFD is based on a combination of these functions and is summarized in \cref{fig:hierarchy}. It was developed by annotating texts in two languages (English and Hindi) for four different genres---namely TED talks, a presidential inaugural speech, news articles, and fictional narratives---keeping in mind the communicative functions that have been associated with definiteness in the linguistic literature. 

CFD is hierarchically organized. This hierarchical organization serves to reduce the number of decisions that an annotator needs to make for speed and consistency. 
We now highlight some of the major distinctions in the hierarchy.

At the highest level, the distinction is made between {\bf Anaphora}, {\bf Nonanaphora}, and {\bf Miscellaneous} 
functions of an NP (the annotatable unit). {\bf Anaphora} and {\bf Nonanaphora} respectively describe 
whether an entity is old or new in the discourse; 
the {\bf Miscellaneous} function is mainly assigned to various kinds of nonreferential NPs. 

The {\bf Anaphora} category has two subcategories: {\bf Basic\_Anaphora} and {\bf Extended\_Anaphora}. 
{\bf Basic\_Anaphora} applies to NPs referring to entities that have been mentioned before. 
{\bf Extended\_Anaphora} applies to any NP whose referent has not been mentioned itself, but is evoked by a previously mentioned entity. 
For example, after mentioning a wedding, {\em the bride}, {\em the groom}, and {\em the cake} 
are considered to be {\bf Extended\_Anaphora}.  

Within the {\bf Nonanaphora} category, a first distinction is made between {\bf Unique}, {\bf Nonunique}, and {\bf Generic}. 
The {\bf Unique} function applies to NPs whose referent becomes unique in a context for any of several reasons. 
For example, {\em Obama} can safely be considered unique in contemporary political discourse in the United States. 
The function {\bf Nonunique} applies to NPs that start out with multiple possible referents 
and that may or may not become identifiable in a speech situation. 
For example, {\em a little riding hood of red velvet} in \cref{fig:excerpt} could be annotated with the label {\bf Nonunique}. 
Finally, {\bf Generic} NPs refer to classes or types of entities rather than specific entities. 
For example, {\em Dinosaurs} in {\em Dinosaurs are extinct.} is a {\bf Generic} NP. 

Another important distinction CFD makes is between {\bf Hearer\_Old} 
for references to entities that are familiar to the hearer (e.g., if they are physically present in the speech situation), 
versus {\bf Hearer\_New} for nonfamiliar references. 
This distinction cuts across the two subparts of the hierarchy, {\bf Anaphora} and {\bf Nonanaphora}; 
thus, labels marking {\bf Hearer\_Old} or {\bf Hearer\_New} also encode other distinctions (e.g., {\bf Unique\_Hearer\_Old}, {\bf Unique\_Hearer\_New}, {\bf Nonunique\_Hearer\_Old}).
For further details on the annotation scheme, see \cref{fig:hierarchy} and \citet{bhatia14}.

Because the ordering of distinctions determines the tree structure of the hierarchy, 
the same communicative functions could have been organized in a superficially different way.
In fact, \citet{komen-13} has proposed a hierarchy with similar leaf nodes, but different internal structure. 
Since it is possible that some natural groupings of labels are not reflected in the hierarchy we used, 
we also decompose each label into fundamental communicative functions, which we call \emph{attributes}. 
Each label type is associated with values for attributes  
\uline{A}naphoric, \uline{B}ridging, \uline{F}amiliar, \uline{G}eneric, \uline{P}redicative, \uline{R}eferential, \uline{S}pecific, and \uline{U}nique.
%(to arrive at such natural groupings), viz.: 
These attributes can have values of $+$, $-$, or $0$, as shown in \cref{fig:hierarchy}. 
For instance, with the \uline{A}naphoric attribute, a value of $+$ applies to labels that can never mark NPs new to the discourse, $-$ applies to labels that can {\em only} apply if the NP is new in the discourse,  
and $0$ applies to labels such as {\bf Pleonastic} (where anaphoricity is not applicable because there is no discourse referent). 



\section{Data}\label{sec:data}

We use the English definiteness corpus of \citet{bhatia14}, 
which consists of texts from multiple genres annotated with the scheme described in \cref{sec:scheme}.\footnote{The data can be obtained from \url{http://www.cs.cmu.edu/~ytsvetko/definiteness_corpus}.} 
The 17~documents consist of prepared speeches (TED talks and a presidential address), 
published news articles, and fictional narratives. 
The TED data predominates (75\% of the corpus);\footnote{The TED talks are from a large parallel corpus obtained from \url{http://www.ted.com/talks/}.} 
the presidential speech represents about 16\%, fictional narratives 5\%, and news articles 4\%. 
All told, the corpus contains 13,860~words (868~sentences), with 3,422~NPs (the annotatable units). 
\citet{bhatia14} report high inter-annotator agreement, estimating Cohen's $\kappa = 0.89$ within the TED genre 
as well as for all genres.  

\Cref{fig:excerpt} is an excerpt from the ``Little Red Riding Hood'' annotated with the CFD scheme.


\begin{figure*}[t]\small
\textit{Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child.}
\\[5pt]
Once \glosst{she}{\llbl{Same\_Head}} gave \glosst{her}{\llbl{Different\_Head}} 
\glosst{a little riding hood of \glosst{red velvet}{\llbl{Other\_Nonreferential}}}{$\underset{~}{\llbl{Nonunique\_Hearer\_New\_Spec}}$}, 
which suited \glosst{her}{\llbl{Same\_Head}} so well that \glosst{she}{\llbl{Same\_Head}} 
would never wear \glosst{anything else}{\llbl{Quantified}}; so \glosst{she}{\llbl{Same\_Head}} was always called 
`\glosst{Little Red Riding Hood}{\llbl{Unique\_Hearer\_New}}.'
\caption{An annotated sentence from ``Little Red Riding Hood.'' The previous sentence is shown for context.}
\label{fig:excerpt}
\end{figure*}





\section{Classification framework}\label{sec:modeling}

To model the relationship between the grammar of definiteness and its communicative functions in a data-driven fashion,
we work within the supervised framework of feature-rich discriminative classification, 
treating the functional categories from \cref{sec:scheme} as output labels $y$
and various lexical, morphological, and syntactic characteristics of the language as features of the input $x$.
Specifically, we learn two kinds of probabilistic models. 
The first is a log-linear model similar to multiclass logistic regression, 
but deviating in that
logistic regression treats each output label (response) as atomic, whereas 
we decompose each into \emph{attributes} based on their linguistic definitions, 
enabling commonalities between related labels to be recognized.
Each weight in the model corresponds to a feature that mediates between 
\emph{percepts} (characteristics of the input NP) and attributes (characteristics of the label). %\ab{do we want to explain what features are, or is it obvious? I don’t have background knowledge about this. I mean it is clear from the above statement that features and percepts refer to different things but due to the terminology, elsewhere it may be confusing when we use one or the other of these two terms. Do we have any references for the difference between features and percepts? }
\costversion{\nss{}the following ways:
\begin{itemize}
  \item Logistic regression treats each output label (response) as atomic; 
  we decompose each into \emph{attributes} based on their linguistic definitions, 
  enabling commonalities between related labels to be recognized.
  Each weight in the model corresponds to a feature that mediates between 
  \emph{percepts} (characteristics of the input NP) and attributes (characteristics of the label).
  \item Logistic regression assumes a prediction is either correct or incorrect.
  We incorporate a \emph{cost function} that gives partial credit during learning when a related label 
  is predicted, so the learned model will better match our evaluation measure.
  \item Logistic regression assumes the space of possible predictions matches 
  the space of labels observed in the training data; we allow more abstract labels to be predicted, 
  which can receive partial credit. The scoring scheme encourages the predictor to ``back off'' 
  to a coarser label if it is not sufficiently confident about a fine-grained label.
\end{itemize}
These decisions are\nss{}}
This is aimed at attaining better predictive accuracy 
as well as feature weights that better describe the form--function interactions we are interested in recovering.
We also train a random forest model on the hypothesis that 
it would allow us to sacrifice interpretability of the learned parameters for predictive accuracy.

Our setup is formalized below, where we discuss the mathematical models and linguistically motivated features.

\subsection{Models}\label{sec:modelslnl}

We experiment with two classification methods: a log-linear model and a nonlinear tree-based ensemble model. 
Due to their consistency and interpretability, linear models are a valuable tool 
for quantifying and analyzing the effects of individual features. 
Non-linear models, while less interpretable, often outperform logistic regression \citep{PerlichEtAl:2003}, 
and thus could be desirable when the predictions are needed for a downstream task.

\subsubsection{Log-linear model}

At test time, we model the probability of communicative function label $y$ 
conditional on an NP $x$ as follows:
\begin{equation}
p_{\boldsymbol{\theta}}(y | x) = \log{\frac{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y)}}{\sum_{y' \in \mathcal{Y}}\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y')}}}
\end{equation}\finalversion{\cjd{Wouldn't it make more sense, given the story about percepts and label attributes, to use the form $\boldsymbol{\phi}(x)^{T} W \boldsymbol{\omega}(y)$? Then you could cast $W$ as a linear map from the percept vector space to the attribute vector space. This is a bit different than the usual linear regression story (of course), but it's closer to what you're actually doing. I think the links to the feature vector function of $x,y$ is a bit opaque and could be a footnote, but I'll leave it up to you.}}
where $\boldsymbol{\theta} \in \mathbb{R}^d$ is a vector of parameters (feature weights), and 
$\mathbf{f}: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^d$ is the feature function over input--label pairs.
The feature function is defined as follows:
\begin{equation}
\mathbf{f}(x,y) = \boldsymbol{\phi}(x) \times \tilde{\boldsymbol{\omega}}(y)
\end{equation}
where the percept function $\boldsymbol{\phi}: \mathcal{X} \rightarrow \mathbb{R}^c$ 
produces a vector of real-valued characteristics of the input, and  
the attribute function\finalversion{\fta{define a and c to make it clear}} $\tilde{\boldsymbol{\omega}}: \mathcal{Y} \rightarrow \{0,1\}^a$
encodes characteristics of each label.
There is a feature for every percept--attribute pairing: so
$d = c \cdot a$ and $f_{(i-1)a+j}(x,y) = \phi_i(x)\tilde{\omega}_j(y), 1 \leq i \leq c, 1 \leq j \leq a$.\footnote{\Citet{chahuneau-13} 
use a similar parametrization for their model of morphological inflection.}
The contents of the percept and attribute functions are detailed in \cref{sec:feats,sec:attrs} respectively.

For prediction, having learned weights $\hat{\boldsymbol{\theta}}$ we use the Bayes-optimal decision rule for minimizing misclassification error, 
selecting the $y$ that maximizes this probability:
\begin{equation}
\hat{y} \leftarrow \arg\max_{y \in \mathcal{Y}} p_{\hat{\boldsymbol{\theta}}}(y | x)
\end{equation}
Training optimizes $\hat{\boldsymbol{\theta}}$ so as to maximize a convex $L_2$-regularized\footnote{As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are excluded from regularization.}
\costversion{\emph{softmax-margin} }learning objective\costversion{ \citep{gimpel}} over the training data $\mathcal{D}$:
\begin{equation}
\hat{\boldsymbol{\theta}} = \argmax_{\boldsymbol{\theta}} 
-\lambda ||\boldsymbol{\theta}||^2_2 
+ \sum_{\langle x,y \rangle\in\mathcal{D}} \log{\frac{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y)}}{\sum_{y' \in \mathcal{Y}}\exp{\left(\boldsymbol{\theta}^{\top}\mathbf{f}(x,y')\costversion{ + \kappa\textit{cost}(y,y')}\right)}}}
\end{equation}
\costversion{The \emph{cost function} allows us to penalize some errors more than others during training, 
taking into account the linguistic functions of the labels.
It is zero for the gold label and nonnegative for the others.\nss{intuition}

This framework gives us several ways to design a classifier appropriate to the task: 
the attributes, the space of labels $\mathcal{Y}$ to consider, 
and the cost function, and of course, the features themselves.}
With \costversion{$\kappa = 0$, $\mathcal{Y} = \{\textit{gold labels in training}\}$, 
and }$\tilde{\boldsymbol{\omega}}(y) = \textit{the identity of the label}$, 
this reduces to standard logistic regression.

\subsubsection{Non-linear model}

We employ a random forest classifier \citep{Breiman:2001}, an ensemble of decision tree classifiers 
%${h(\mathbf{x}, N, \boldsymbol{\Theta})}$, 
%where $\mathbf{x}$ is an input vector, $N$ is the number of classification trees, and $\boldsymbol{\Theta}$ is a set of $N$ independently generated random vectors. 
%The random vector for the $k$th tree---$\Theta_k$---is generated independently from the past $\Theta_1, \dots, \Theta_{k-1}$ vectors, but from the same distribution. 
%\nss{they come from the same distribution---what is it?}
%Each random vector governs the growth of its tree in the ensemble. 
learned from many independent subsamples of the training data.
%After $N$ trees are generated, the most popular class is selected by voting; each tree casts a unit vote.
Given an input, each tree classifier assigns a probability to each label; 
those probabilities are averaged to compute the probability distribution across the ensemble.
 
An important property of the random forests, in addition to being an effective tool in prediction, is their immunity to overfitting: 
as the number of trees increases, they produce a limiting value of the generalization error.\footnote{See Theorem 1.2 in \cite{Breiman:2001} for details.} 
Thus, no hyperparameter tuning is required. %; this alleviates the scarcity problem in setups with limited amount of data.
Random forests are known to be robust to sparse data and to label imbalance \citep{chen}, both of which are challenges with the definiteness dataset.

\subsection{Percepts}\label{sec:feats}

The characteristics of the input that are incorporated in the model, which we call \emph{percepts} 
to distinguish them from model features linking inputs to outputs,\footnote{See \cref{sec:modelslnl}} 
are intended to capture the aspects of English morphosyntax that may be relevant 
to the communicative functions of definiteness.

After preprocessing the text with a dependency parser and coreference resolver, which is described in \cref{sec:exptsetup}, 
we extract several kinds of percepts for each NP.

\subsubsection{Basic}

\paragraph{Words of interest.} 
These are the \emph{head} within the NP, all of its \emph{dependents}, and its \emph{governor} (external to the NP). 
We are also interested in the \emph{attached verb}, which is the first verb one encounters when traversing the dependency path upward from the head. 
For each of these words, we have separate percepts capturing: the token, the part-of-speech (POS) tag, the lemma, 
the dependency relation,
and (for the head only) a binary indicator of plurality (determined from the POS tag).
As there may be multiple dependents, we have additional features specific to the first and the last one. 
Moreover, to better capture tense, aspect and modality, we collect the attached verb's \emph{auxiliaries}. 
We also make note of the negative particle (with dependency label \emph{neg}) if it is a dependent of the verb.
    
\paragraph{Structural.} 
The structural percepts are: the \emph{path length} from the head up to the root, and to the attached verb. 
We also have percepts for the number of dependents, and the number of dependency relations that link non-neighbors.
Integer values were binarized with thresholding.

\paragraph{Positional.} 
These percepts are the \emph{token length} of the NP, the NP's \emph{location} in the sentence (first or second half), and 
the \emph{attached verb's position} relative to the head (left or right). 
12~additional percept templates record the POS and lemma of the left and right neighbors of the head, governor, and attached verb.

\subsubsection{Contextual NPs}

When extracting features for a given NP (call it the ``target''), 
we also consider NPs in the following relationship with the target NP:
its \emph{immediate parent}, which is the smallest NP whose span fully subsumes that of the target; 
the \emph{immediate child}, which is the largest NP subsumed within the target;
the \emph{immediate precedent} and \emph{immediate successor} within the sentence; 
and the \emph{nearest preceding coreferent mention}.

For each of these related NPs, we include all of their basic percepts 
conjoined with the nature of the relation to the target.

\subsection{Attributes}\label{sec:attrs}

As noted above, though CFD labels are organized into a tree hierarchy, 
there are actually several dimensions of commonality that suggest different groupings.
These attributes are encoded as ternary characteristics; 
for each label (including internal labels), every one of the 8 attributes  
is assigned a value of $+$, $-$, or $0$ (refer to \cref{fig:hierarchy}).
In light of sparse data, we design features\costversion{ and cost function} to exploit these similarities 
via the attribute vector function
\begin{equation}
\boldsymbol{\omega}(y) = [y, A(y), B(y), F(y), G(y), P(y), R(y), S(y), U(y)]^{\top}
\end{equation}
where $A: \costversion{\mathcal{L} \cup \mathcal{I}\nss{}}\mathcal{Y} \rightarrow \{+, -, 0\}$ returns the value for Anaphoric, 
$B(y)$ for Bridging, etc.
The identity of the label is also included in the vector so that 
different labels are always recognized as different by the attribute function.
The categorical components of this vector are then binarized to form $\tilde{\boldsymbol{\omega}}(y)$; 
however, instead of a binary component that fires for the $0$ value of each ternary attribute, 
there is a component that fires for \emph{any} value of the attribute---a sort of bias term.
The weights assigned to features incorporating $+$ or $-$ attribute values, then, 
are easily interpreted as deviations relative to the bias.

\costversion{\subsection{Cost and label space}\label{sec:cost}

The definiteness function hierarchy presented in \cref{fig:hierarchy} 
consists of 24~\emph{leaf labels}, which will be denoted $\mathcal{L}$, and 
10~more abstract \emph{intermediate labels}, denoted $\mathcal{I}$.
All of the gold labels in the training data are from $\mathcal{L}$, 
but we give our model the option to predict more abstract labels 
to receive partial credit.
We will therefore use $\mathcal{Y} = \mathcal{L} \cup \mathcal{I}$.

The relatedness of label pairs depends on the number of values that differ between the two original attribute vectors: 
let $\Delta(y,y') = |\boldsymbol{\omega}(y) \ominus \boldsymbol{\omega}(y')|$.\footnote{By the symmetric difference of two attribute vectors, we mean the subset of components that lack a matching (categorical) value.} 
For a gold leaf label $\ell$ and an internal label $\iota$ that subsumes it in the hierarchy 
(i.e., $\iota$ is an ancestor of $\ell$), let $\delta(\ell,\iota) =$ the distance in the hierarchy 
between $\ell$ and $\iota$.\footnote{There is no need to define $\delta(\iota, \cdot)$, as the training set does not contain intermediate labels.}
Now define $\textit{cost}(y,y') = \Delta(y,y') - 0.5^{\delta(y,y')}$: 
this assigns lower (better) cost to an internal label than to an incorrect leaf label with an equivalent attribute distance.}

\section{Evaluation}\label{sec:eval}

The following measures are used to evaluate our predictor against the gold standard 
for the held-out evaluation (dev or test) set $\mathcal{E}$:
\begin{itemize}
  \item \textbf{Exact match:} This accuracy measure gives credit only where the predicted and gold labels 
  are identical.\costversion{\nss{} When the model is allowed to predict internal labels, we will report 
  overall precision and recall of leaf labels. Otherwise, we report accuracy.}
  \item \textbf{By leaf label:} We also compute precision and recall of each leaf label 
  to determine which categories are reliably predicted.
  \item \textbf{Soft match:} This accuracy measure gives partial credit where the predicted and gold labels 
  are related. It is computed as the proportion of attributes-plus-full-label whose (categorical) values match: 
  \mbox{$|\boldsymbol{\omega}(y) \cap \boldsymbol{\omega}(y’)|/9$}.\finalversion{\cjd{Should these be differentially weighted based on their height in the hierarchy?}}
%   \item \textbf{Perplexity:} Perplexity is a value $\rho \in [1,\infty)$ that quantifies how ``surprised'', on average, our model is by the gold labels 
%   in the test set; the greater the probability mass assigned to the true labels, 
%   the lower the score. 
%   It is computed as $\rho = 2^{-\left(\sum_{\langle x, y \rangle \in \mathcal{E}} \log_2 p_{\hat{\boldsymbol{\theta}}}(y \mid x)\right) / |\mathcal{E}|}$. Intuitively a perplexity of $\rho$ means the amount of surprisal in a $\rho$-way uniform random draw.
\end{itemize}

\section{Experiments}\label{sec:exp}

\subsection{Experimental Setup}\label{sec:exptsetup}

\paragraph{Data splits.} The annotated corpus of \citet{bhatia14} (\cref{sec:data}) contains 17~documents 
in 3~genres: 13~prepared speeches (mostly TED talks),\footnote{We have combined the TED talks and presidential speech genres since both involved prepared speeches.} 2~newspaper articles, 
and 2~fictional narratives. We arbitrarily choose some documents to hold out from each genre; 
the resulting test set consists of 2~TED talks (``Alisa\_News'', ``RobertHammond\_park''), 
1~newspaper article (``crime1\_iPad\_E''), and 1~narrative (``Little Red Riding Hood'').
The test set then contains 19,28~tokens (111~sentences), in which there are 511~annotated NPs; 
while the training set contains 2,911~NPs among 11,932~tokens (757~sentences).

\paragraph{Preprocessing.} Automatic dependency parses and coreference information were obtained with 
the parser and coreference resolution system in Stanford CoreNLP v.~3.3.0 \citep{socher-13,recasens-13}
for use in features (\cref{sec:feats}). Syntactic features were extracted from the Basic dependencies output by the parser. 
To evaluate the performance of Stanford system on our data, 
we manually inspected the dependencies for a subset of sentences from our corpus (using texts from TED talks and fictional narratives genres) 
and recorded the errors. 
We found that about 70\% of the sentences had all correct dependencies, and only about 0.04\% of the total dependencies were incorrect for our data.

Throughout our experiments (training as well as testing), we use the gold NP boundaries identified by the human annotators (gold NP boundaries).
The automatic dependency parses are used to extract percepts for each gold NP.     
If there is a conflict between the gold NP boundaries and the parsed NP boundaries, to avoid extracting misleading percepts, 
we assign a default value.  
For example, some of our percepts are properties of the ``outbound head'', the immediate parent node to which the NP under consideration is attached.
We assume the gold NP boundary and look for words within this boundary which have an outbound dependency (a link to a word outside the NP boundary).    
If there is a conflict between the gold boundary  and the parser-generated boundary or if multiple words with outbound dependencies are found, then
we select the word from within the gold boundary which is linearly closest to its outbound head.

\paragraph{Learning.} The log-linear model variants are trained with an in-house implementation of supervised learning with $L_2$-regularized AdaGrad \citep{adagrad}.
Hyperparameters are tuned  
on a development set formed by holding out every tenth instance from the training set 
(test set experiments use the full training set):
the power of 10 giving the highest soft match accuracy was chosen for $\lambda$.\footnote{Preliminary experiments with cross-validation on the training data showed that the value of $\lambda$ was stable across folds.}
The Python {\tt scikit-learn} toolkit \citep{scikit-learn} was used for the random forest classifier.\footnote{Because it is a randomized algorithm, the results may vary slightly between runs; 
however, a cross-validation experiment on the training data found very little variance in accuracy.}




\begin{table*}\small\centering
\begin{tabular}{lrrcc}
\multicolumn{1}{c}{\bf Condition} & \multicolumn{1}{c}{$|\boldsymbol{\theta}|$} %& \multicolumn{1}{c}{$||\boldsymbol{\theta}||_0$} 
& \multicolumn{1}{c}{$\lambda$} 
& \multicolumn{1}{c}{\bf Exact Match Acc.} & \multicolumn{1}{c}{\bf Soft Match Acc.} 
%& \multicolumn{1}{c}{\bf Perplexity} 
\\
\midrule
Majority baseline & \multicolumn{1}{c}{---} & \multicolumn{1}{c}{---} %& \multicolumn{1}{c}{---} 
& 12.1 & 47.8
\\
Log-linear classifier, attributes only & 473,064 & 100 & 38.7 & 77.1 \\
Log-linear classifier, labels only & 413,931 & 100 & 40.8 & 73.6  \\
Full log-linear classifier (labels + attributes) & 926,417 & 100 & 43.7 & \bf 78.2 \\
Random forest classifier & 20,363 & \multicolumn{1}{c}{---} & \bf 49.7 & 77.5 \\

\end{tabular}
\caption{Classifiers and baseline, as measured on the test set.
%The first three columns of numbers report the number of parameters (feature weights), the number of nonzero parameters, 
%and the tuned regularization hyperparameter, respectively.
The first two columns give the number of parameters and the tuned regularization hyperparameter, respectively; 
the third and fourth columns give accuracies as percentages; the best in each column is bolded.}
\label{tbl:results}
\end{table*}




\subsection{Results}

\begin{table*}\small\centering
   \begin{tabular}{lrrrr}
      \bf{Leaf label} & \bf{No. of Instances} & \bf{Prec.} & \bf{Recall} & \bf{$F_1$} \\
      \midrule
Nonunique\_Predicative\_Identity & 10 & --- & 0 & --- \\
Pleonastic & 44 & 100 & 78 & 88 \\
Nonunique\_Nonspec & 173 & 9 & 25 & 13 \\
Other\_Nonreferential & 134 & 39 & 36 & 37 \\
Nonunique\_Larger\_situation & 97 & 29 & 25 & 27 \\
Bridging\_Other\_Context & 96 & 33 & 6 & 11 \\
Same\_Head & 452 & 41 & 41 & 41 \\
Unique\_Hearer\_New & 26 & --- & 0 & --- \\
Quantified & 213 & 57 & 57 & 57 \\
Different\_Head & 271 & 32 & 33 & 32 \\
Bridging\_Nominal & 33 & 33 & 10 & 15 \\
Measure\_Nonreferential & 98 & 88 & 26 & 40 \\
Part\_of\_Noncompositional\_MWE & 88 & 20 & 17 & 18 \\
Bridging\_Restrictive\_Modifier & 552 & 58 & 84 & 68 \\
Nonunique\_Hearer\_New\_Spec & 190 & 36 & 46 & 40 \\
Predicative\_Nonidentity & 57 & 0 & 0 & --- \\
Unique\_Larger\_situation & 97 & 52 & 58 & 55 \\
Generic\_Individual\_Level & 113 & 14 & 11 & 13 \\
Bridging\_Event & 9 & --- & 0 & --- \\
Nonunique\_Physical\_Copresence & 36 & 0 & 0 & --- \\

   \end{tabular}
   \caption{Leaf Precision, Recall and F1 as percentages. 
The number of instances in the first column are from the training set.}
   \label{tbl:leaf}
\end{table*}


Measurements of overall classification performance appear in \cref{tbl:results}. While far from perfect, our classifiers achieve promising accuracy levels 
given the small size of the training data and the number of labels in the annotation scheme.
The Random forest classifier is the most accurate in Exact Match measure, likely due to the robustness of that technique under
conditions where the data are small and the frequencies of individual labels are imbalanced. In Soft Match measure, our attribute-aware log-linear models perform very well. The most successful of the log-linear models is the richest model, which combines the fine-grained 
communicative function labels with higher-level attributes of those labels. But notably the attribute-only model,
which breaks down the semantic labels into combination of attributes, does not fall much behind and performs almost as well as the Random forest classifier in Soft Match.
This is encouraging because it suggests that the model has correctly exploited known linguistic generalizations 
to account for the grammaticalization of definiteness in English. 


An advantage of log-linear models is that feature weights offer insights into the model's behavior.
\Cref{fig:weights} lists the 10~features that received the highest positive weights in the full model 
for the \mbox{$+$ and $-$ values} of the Specific attribute. Reassuringly, the definite article, 
possessives (\texttt{PRP\$}), proper nouns (\texttt{NNP}), and the second person pronoun 
are associated with specific NPs, while the indefinite article is associated with nonspecific NPs.
The model also seems to have picked up on the less obvious but well-attested tendency 
of objects to be nonspecific \citep{aissen-03}.
% \footnote{The percept \texttt{VB\_pos\_outerhead\_left}  fires when the NP is governed by a verb to its left.}


In addition to confirming known grammaticalization patterns of definiteness, 
we can mine the highly-weighted features for new hypotheses: 
here the model thinks that objects of ``from'' are especially likely to be specific, 
and that NPs with comparative adjectives (\texttt{JJR}) are especially likely to be nonspecific. For another example, \emph{Num. of dependents, dependent's POS: $1$,PRP\$} has higher weight than, say \emph{Num. of dependents, dependent's POS: $2$,PRP\$}. This could be due to the inherent difference associated with specificity, or frequency difference for the two patterns in the language, or frequency difference for the patterns in our corpus. Whether these are general trends, or just an artifact of the sentences that happened to be in the training data, 
will require further investigation, ideally with additional datasets and more rigorous hypothesis testing.

\begin{figure}\small\centering
   \begin{tabular}{lr|lr}
      \multicolumn{4}{c}{\bf Percepts} \\
\multicolumn{2}{c}{\bf $+$Specific} & \multicolumn{2}{c}{\bf $-$Specific} \\
First dependent's POS & PRP\$ & First dependent's lemma & a \\
Head's left neighbor's POS & PRP\$ & Last dependent's lemma & a \\
Last dependent's lemma & you & Num. of dependents, dependent's lemma & $1$,a \\
Num. of dependents, dependent's lemma & $1$,you & Head's left neighbor's POS & JJR \\
Num. of dependents, dependent's POS & $1$,PRP\$ & Last dependent's POS & JJR \\
Governor's right neighbor's POS & PRP\$ & Num. of dependents, dependent's lemma & $2$,a \\
Last dependent's POS & NNP & First dependent's lemma & new \\
Last dependent's POS & PRP\$ & Last dependent's lemma & new \\
First dependent's lemma & the & Num. of dependents, dependent's POS & $2$,JJR \\
Governor's lemma & from & Governor's left neighbor's POS & VB \\
\end{tabular}
\caption{Percepts receiving highest positive weights in association with attribute values.}
\label{fig:weights}
\end{figure}

Finally, we can remove features to test their impact on predictive performance. 
Notably, in experiments ablating features indicating articles---the most obvious exponents of definiteness 
in English---we see a decrease in performance, but not a drastic one. 
This suggests that the expression of communicative functions of definiteness is in fact much richer than 
morphological definiteness.




\paragraph{Errors.}
Several labels are unattested or virtually unattested in the training data, 
so the models unsurprisingly fail to predict them correctly at test time. 
\llbl{Same\_Head} and \llbl{Different\_Head}, though both common, are confused quite frequently.
Whether the previous coreferent mention has the same or different head is a simple distinction for humans; 
low model accuracy is likely due to errors propagated from coreference resolution.
This problem is so frequent that merging these two categories and retraining the random forest model 
improves exact match accuracy by 8\% absolute and soft match accuracy by 5\% absolute.
Another common confusion is between the highly frequent category \llbl{Unique\_Larger\_Situation}
and the rarer category \llbl{Unique\_Hearer\_New}; the latter is supposed to occur
only for the first occurrence of a proper name referring to a entity 
that is not already part of the knowledge of the larger community. 
In other words, this distinction requires world knowledge about well-known entities, which could perhaps be mined 
from the Web or other sources.

\finalversion{\nss{English: ±cost function, ±non-identity attributes, ±predicting intermediate labels}

\nss{maybe: which attribute groupings produce the best classifier, if we want to force a hierarchy}

\nss{feature/attribute ablations}}


\begin{figure}\small\centering  
   \begin{tabular}{p{.4\textwidth}p{.3\textwidth}p{.2\textwidth}}
      \bf{Example} & \bf{Relevant percepts from \cref{fig:weights}} & \bf{CFD annotation} \\
      \midrule
      This is just for \emph{the United States of America}. & Last dependent's POS: NNP, first dependent's lemma: the & Unique\_Larger\_situation \\ \\
      We were driving from \emph{our home in Nashville} to a little farm we have 50 miles east of Nashville --- driving ourselves. & First dependent's POS: PRP\$, head's left neighbor's POS: PRP\$, governor's right neighbor's POS:PRP\$, governor's lemma: from & Bridging\_restrictiveModifier \\
\end{tabular}
\caption{Sentences from our corpus to illustrate Percepts fired and the corresponding CFD annotations.}
\label{fig:featureweights}
\end{figure}



\section{Related Work}\label{sec:related}

Because semantic\slash pragmatic analysis of referring expressions is important for many NLP tasks,  
a computational model of the communicative functions of definiteness has the potential to leverage 
diverse lexical and grammatical cues to facilitate deeper inferences about the meaning of linguistic input.
We have used a coreference resolution system to extract features for modeling definiteness, but 
an alternative would be to predict definiteness functions as input to (or jointly with) the coreference task. 
Applications such as information extraction and dialogue processing could be expected to benefit not only from 
coreference information, but also from some of the semantic distinctions made in our framework, including specificity and genericity.

Better computational processing of definiteness in different languages stands to help machine translation systems. 
It has been noted that machine translation systems face problems when the source and the target language 
use different grammatical strategies to express the same information \citep{stymne09,tsvetkov13}. 
Previous work on machine translation has attempted to deal with this in terms of either 
(a)~preprocessing the source language to make it look more like the target language \citep[\textit{inter alia}]{collins05,habash07,niessen00,stymne09}; 
or (b)~post-processing the machine translation output to match the target language, \cite[e.g.,][]{popovic06}. 
Attempts have also been made to use syntax on the source and\slash or the target sides 
to capture the syntactic differences between languages \citep{liu06,yamada02,zhang07}. 
Automated prediction of (in)definite articles has been found beneficial in a variety of applications, 
including postediting of MT output \citep{knight1994automated}, text generation \citep{DBLP:conf/aaai/Elhadad93,minnen2000memory}, 
and identification and correction of ESL errors \citep{aehan2006detecting,de2008classifier,gamon2008,rozovskaya2010training}. 
More recently, \citet{tsvetkov13} trained a classifier to predict where English articles might plausibly be added or removed in a phrase, 
and used this classifier to improve the quality of statistical machine translation. 

While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 
studies on additional, more complex aspects of definiteness are limited.  
\citet{reiter10}  exploit linguistically-motivated features in a supervised approach to 
distinguish between generic and specific NPs.\finalversion{\yt{what else?}}
To the best of our knowledge, no studies have been conducted 
on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. 
%An understanding of such communicative functions in a language can come handy, e.g., in cases of 
%syncretism on the source and/or the target side language, where the observable parts of grammar alone cannot guide machine translation systems. 

%\citet{bresnan10} have used logistic regression to predict syntactic structures related to dative alternation based on the semantic mappings to the structures. %\nss{mention Bresnan's work on predicting syntactic alternations with logistic regression 
%(here we want to predict the hidden information so that the classifier is useful for applications\!).}


Our work is related to research in linguistics on the modeling of syntactic constructions 
such as dative shift and the expression of possession with ``of'' or ``'s''.  
\citet{bresnan10} used logistic regression with semantic features to predict syntactic constructions.   
Although we are doing the opposite (using syntactic features to predict semantic categories), 
we share the assumption that reductionist approaches (as mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. 
Following~\citet{hopper-03} we observe that grammaticalization is accompanied by {\em function drift}, 
resulting in multiple communicative functions for each grammatical construction. 
% \ab{Hence, like \citet{bresnan10}, in this work, we used logistic regression models to explore the syntax-semantic mapping 
% related to definiteness since such models are robust to deal with skewed data as well and hence are able to model probability 
% without assuming a particular distribution of data.???!}



\section{Conclusion}\label{sec:conclusion}

We have presented a data-driven approach to modeling the relationship between 
communicative functions associated with (in)definiteness and their lexical\slash grammatical realization in a particular language.
Our feature-rich classifiers can give insight into this relationship
as well as predict communicative functions for the benefit of NLP systems.
This work has focused on English, but in future work we will build similar models for other languages---including 
languages without articles, under the hypothesis that such languages will rely on other, subtler devices to encode
many of the functions of definiteness.




\bibliographystyle{aclnat}
% you bib file should really go here
\setlength{\bibsep}{10pt}
{\fontsize{10}{12.25}\selectfont
\bibliography{definiteness}}


\end{document}


\section{Discussion: Error analysis, comparison across languages}\label{sec:discussion}

\ab{confusion matrix - Hindi vs.~Eng (for common annotations)- semantic annotations
differences in annotations- why- different grammatical constructions, e.g. NP in 1 language but predicate in another etc.
what do similarities tell us about grammaticalization, what do differences tell.
discuss- how this analysis can help in MT}

%A total of 6 annotators contributed to the annotations. Two of the annotators are linguists while others are computer scientists working on languages and an undergraduate student of Linguistics. Two annotators annotated most of the data, the inter annotator agreement between them was very high with Cohen's Kappa score of 0.94 within the TED corpus and 0.91 for combined genres.  However the other 4 annotators also annotated some of the data, but one of the two annotators with most experience took these data as inputs and revised annotations according to their annotations. This two-step process of annotation was taken to increase the amount of data within a short period of time and also to achieve consistency within annotations. All annotators were trained using the annotation guidelines and example annotated texts. Then the annotators were asked to annotate some data, these annotations were discussed to reach at consensus. The annotations used for discussions to reach at consensus were not included while calculating the inter annotator agreement.

\ab{description of data, IAA, other statistics- n(annotations), n(annotations/ category)
classifiers for English- baseline, accuracy etc, confusion matrix- classifier predictions and gold standard
may be a brief discussion of feature ablation study-  which features are most helpful at least in these languages corresponding to individual categories (a clue to grammaticalization)
Principle component analysis, clustering of features}

\fta{not totally clear how this relates to learning the communicative functions of definiteness, 
either elaborate or remove the part that start with In contrast to the reductionist...until prototypical functions. 
The part that follows in the previous version is relevant: Grammaticalization varies a lot across ...}

\nss{Hindi?}
