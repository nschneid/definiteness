\PassOptionsToPackage{usenames}{color}
\documentclass[11pt,letterpaper]{article}
\usepackage{comment}
\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{acl2014}

\usepackage[linkcolor=blue]{hyperref}

\usepackage[round]{natbib}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})}

%\usepackage{times}
%\usepackage{latexsym}


\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage[procnames]{listings}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

\usepackage{mathptmx}	% txfonts
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}





% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}


\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
\usepackage{tikz}
%\usepackage{tree-dvips}
\usetikzlibrary{arrows,positioning,calc} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{verbatim}

% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
\makeatletter
\lst@AddToHook{EveryPar}{%
  \label{lst:\thelstnumber}% make a label for each line number except the first (assumes only one listing in the document)
}
\makeatother
\lstset{
% basicstyle=\rmshape,
  numbers=left,
  numberstyle=\tt\color{gray},
  firstnumber=2,
  stepnumber=5,
  xleftmargin=3em,
  language=Python,
  upquote=true,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=1,
  stringstyle=\color{mdgreen},
  commentstyle=\itshape\color{lavender},
  basicstyle=\small\smaller\ttfamily,
  morekeywords={lambda,with,as,assert},
  keywordstyle=\bfseries\color{magenta},
  procnamekeys={def},
  procnamestyle=\bfseries\color{orange},
  aboveskip=0.5cm,
  belowskip=0.5cm
}
\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\abmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{A}}_{\textsc{B}}}}}}
\newcommand{\ytmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{Y}}_{\textsc{T}}}}}}
\newcommand{\ftamarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{FT}}_{\textsc{A}}}}}}
\newcommand{\lslmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{LS}}_{\textsc{L}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\ab}[1]{\arkcomment{\abmarker}{#1}{red}}
\newcommand{\yt}[1]{\arkcomment{\ytmarker}{#1}{blue}}
\newcommand{\fta}[1]{\arkcomment{\ftamarker}{#1}{orange}}
\newcommand{\lsl}[1]{\arkcomment{\lslmarker}{#1}{purple}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\usepackage{cleveref}

% use \S for all references to all kinds of sections, and \P to paragraphs
% (sadly, we cannot use the simpler \crefname{} macro because it would insert a space after the symbol)
\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
\crefmultiformat{part}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{chapter}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{section}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subsection}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subsubsection}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
% for \label[appsec]{...}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{enums}{example}{examples}
\Crefname{enums}{Example}{Examples}
\crefformat{enums}{(#2#1#3)}

\ifx\creflastconjunction\undefined%
\newcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\else%
\renewcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\fi%

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
\newcommand{\fnref}[1]{\autoref{#1}} % don't use \cref{} due to bug in (now out-of-date) cleveref package w.r.t. footnotes


% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
%\addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
%\addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
%\addtolength{\abovedisplayskip}{-.5cm} % space before maths
%\addtolength{\belowdisplayskip}{-.5cm} % space after maths
%\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
%\setlength{\belowcaptionskip}{-.25cm}


% customize \paragraph spacing
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother



% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% supersense tag name
\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\uline{\textrm{#1}}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\sys}[1]{\mbox{\textbf{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments
\newcommand{\ilbl}[1]{\mbox{\textbf{\textsc{#1}}}} % internal label
\newcommand{\llbl}[1]{\mbox{\textsc{#1}}} % leaf label

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\tat}[0]{\textasciitilde}

\newcommand{\shortlong}[2]{#1} % short vs. long version of the paper
\newcommand{\confversion}[1]{#1}
\newcommand{\srsversion}[1]{}
%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\costversion}[1]{}
\newcommand{\shortversion}[1]{#1}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{} % ...if only there were more space...
\newcommand{\subversion}[1]{#1} % for the submission version only
\newcommand{\draftnotice}[1]{{\it\small #1}\\} % for the draft version only
%\newcommand{\subversion}[1]{}

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{SemCor}
\hyphenation{PennConverter}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}
\hyphenation{per-cept}
\hyphenation{per-cepts}
\title{\draftnotice{Title, author list not final. Page limit: 5 + 2 bib}A Classifier for Communicative Functions of Definiteness}

\finalversion{\author{Archna Bhatia \ \ \  Chu-Cheng Lin \ \ \  Lori Levin \ \ \ \  Mandy Simons \\
\bf Fatima Talib Al-Raisi \ \ \  Laleh Roostapour \ \ \  Abhimanu Kumar  \\
\bf  Nathan Schneider \ \ \  Yulia Tsvetkov \ \ \  Jordan Bender \ \ \  Chris Dyer\\
Carnegie Mellon University\\
Pittsburgh, PA 15213}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
% Definiteness is a nonhomogeneous category across languages expressing various {\em communicative functions} 
% (types of semantic, pragmatic, and discourse--related information).   
% Languages differ in the grainsize and inventory of communicative functions  that are expressed 
% as well as the syntactic means for expressing them.  
% \citet{bhatia14} presented an annotation scheme for the communicative functions of definiteness 
% and released an annotated corpus in English and Hindi.   
Whether a noun phrase is realized grammatically as definite or not 
depends on a variety of semantic, pragmatic, and discourse criteria, or {\em communicative functions}, 
the interaction of which varies from language to language.
This paper reports on a supervised classifier for English that uses lexical, morphological, and syntactic features 
to predict communicative functions of definiteness.   
The benefits of this work are twofold: linguistically, the classifier's features and weights 
model the {\em grammaticalization} of definiteness in English, 
not all of which are obvious.
% some of which is obvious (e.g., {\em the} and {\em a}) and some of which is not obvious \ab{(include an example from our findings)}. 
Computationally, it presents a framework to predict \textit{semantic and pragmatic} communicative functions of definiteness 
which, unlike lexical and morphosyntactic features, are preserved in translation.  
The classifier may also be useful in tracking noun phrase reference for 
%information extraction, entity tracking, and other 
semantic processing tasks. 
\end{abstract}

\section{Introduction}

Languages display a vast range of variation with respect to the form and meaning of definiteness. 
For example, while languages like English make use of definite and indefinite articles 
to distinguish between the discourse status of various entities ({\it the car} vs.~{\it a car}), 
many other languages, such as Czech, Hindi, Indonesian, and Russian, do not have articles 
(although they do have demonstrative determiners).  
Some languages such as Hausa~\cite{lyons} have different definite articles for noun phrases 
that have been previously mentioned in contrast to those that are definite by virtue of the situation 
(e.g., ``the podium'' at a conference).  
Definiteness can also be expressed by affixes as in Arabic.  
\citet{chen04} shows that Chinese, a language without articles, expresses (in)definiteness through constructions, 
such as the existential construction for indefinite subjects and the {\it ba}- construction for definite direct objects. 
Demonstratives, personal pronouns and possessives (which are found in all languages) \fta{Demonstrative, personal pronouns, and ... (which are found in all languages)
I suggest removing (which are found in all languages) since the statement may give the impression that these are syntactic phenomena in all languages which is not always the case (Arabic for example expresses possessives in a number of different ways none of which is identified explicitly as a grammaticalization of possessive)} are other kinds of definite NPs. 

Aside from this variation in the form of (in)definite NPs within and across languages, there is also variability 
in the semantic, pragmatic, and discourse--related functions (in)definites expresses.   
We will refer to these as {\em communicative functions} \fta{of (in)definiteness}.  
The literature on definiteness suggests that it may express communicative functions such as 
uniqueness, familiarity, identifiability, anaphoricity, specificity, and 
referentiality \citep[\textit{inter alia}]{birner94,condoravdi92,evans77,evans80,gundel88,gundel93,heim90,kadmon87,kadmon90,lyons99,prince92,roberts03,russell05}.  

Reductionist approaches to definiteness try to define one or two communicative functions of definiteness.   
For example, \citet{kadmon87,evans80} propose that semantic uniqueness is the main communicative function of definite NPs.  
\citet{roberts03} proposes that the combination of uniqueness and a presupposition of familiarity 
underlie all definite descriptions.  However, possessive definite descriptions ({\it John's daughter}) 
and the weak definites ({\it the son of Queen Juliana of the Netherlands}) are neither unique nor necessarily 
familiar to the listener before they are spoken.  

We take such linguistic observations to suggest that definiteness is not as homogeneous a category 
as many accounts have assumed.  In contrast to the reductionists, we are following an 
approach to grammaticalization~\citep{?} in which grammar develops over time in such a way that 
each grammatical construction has some prototypical communicative functions, 
but also has many non-prototypical communicative functions\fta{not totally clear how this relates to learning the communicative functions of definiteness, either elaborate or remove the part that start with In contrast to the reductionist...until prototypical functions. The part that follows in the previous version is relevant: Grammaticalization varies a lot across ...}.  

This paper describes a classifier that predicts communicative function labels for English noun phrases.  
The features used by the classifier are lexical, morphological, and syntactic.   
The contribution of our work is in both the output of the classifier and the model that it uses (features and weights).  
The classifier outputs communicative function labels (see next section, e.g., whether the entities are 
old or new to the discourse and to the hearer).   
Communicative function is important because it is usually preserved in translation even when the grammatical mechanisms 
for expressing it are different. The communicative function labels also represent the discourse status of entities, 
making them relevant for entity tracking, knowledge base construction, and information extraction. 

The model is a form-meaning mapping, consisting of the syntactic, lexical, and morphological features 
and weights that correlate with\fta{suggestion: remove correlate with}---are predictive of---communicative functions.   
This in itself is linguistically significant in that it shows the grammatical mechanisms 
beyond the articles {\em the} and {\em a} that are used for expressing definiteness in English.   
In future work we will build such models for languages that do not have articles such as Hindi, Russian, and Chinese.
The form-meaning mapping for these languages can be used for machine translation applications. 
It has been noted previously that machine translation systems face problems 
while translating from one language to another when the languages use different grammatical strategies. 
\citet{tsvetkov13,stymne09} mention how translating from an article-language to an article-less language is problematic. 
If the mapping between different forms and the information they encode is known
in both the source language and the target language, this information can be leveraged in 
improving machine translation across these languages.  

To build our model, we leverage a cross-lingual definiteness annotation scheme (\cref{sec:scheme}) 
and annotated English corpus (\cref{sec:data}) from prior work \citep{bhatia14}.
Our classifier, \cref{sec:modeling}, is a supervised log-linear model akin to logistic regression,
with features that combine lexical and morphosyntactic information 
with prespecified groupings of the communicative function labels; 
the evaluation measures (\cref{sec:eval}) include one that exploits these label groupings 
to award partial credit according to relatedness.
\nss{TODO: \cref{sec:exp} obtain good performance? discover interesting features?}

%\ab{Here we discuss about why we should study definiteness- linguistically a hard problem, also it has applications in machine translation. Discuss about grammaticalization as a general problem (differences across languages), then grammaticalization of definiteness across languages, some examples to show differences across languages.}
%\nss{we review the annotation scheme in \cref{sec:scheme}; etc.}

\section{Annotation scheme}\label{sec:scheme}
Based on the literature on definiteness, we compiled a list of semantic, pragmatic, and discourse--related 
features which are relevant for the form of (in)definite NPs in English (and many other languages).\yt{ref to the LREC paper?} These features were put together in a hierarchical structure to ease the annotation effort for the annotators. At each step, the annotators make a decision and thus reduce the number of labels they have to consider before annotating an NP. This approach also increases the consistency in annotations due to the reduced effort on considering the options among which to select the correct label for the NP. The annotation scheme was revised through many iterations after attempts at annotating natural language data from different genres using the scheme. The current scheme is presented in Figure~\ref{fig:hierarchy}. 

The first distinction annotators make is whether the NP at hand is anaphoric (discourse old) or non anaphoric (discourse new). 
We do not annotate for uniqueness, hearer old/new distinction, specificity or genericity for the anaphoric NPs 
as those distinctions were deemed inconsequential for English anaphoric NPs\footnote{However, in future, we may include these features for anaphoric NPs as well if contrary evidence were found.}.
\yt{For someone who is not familiar with the scheme, this would be very hard to understand without looking at the definiteness scheme. I think the scheme should be on this page, and description should always point to the place in the scheme. May be we can write all scheme-related terms using the same font as they appear in the scheme? }
The category of anaphoric NPs is further subdivided into basic anaphora and extended anaphora. 
The basic anaphora is used for entities which have been mentioned previously in the discourse. 
The extended anaphora is inspired by the bridging references \citet{clark77} had mentioned. 
These are used for entities which are not discourse old or even hearer old, but at the same time, 
they are not entirely new either.  See an example in \cref{fig:bridging} (borrowed from \citet{poesio98}). 

\begin{figure}\small
(2) I went to {\it a wedding} last weekend. {\it The bride} was a friend of mine. She baked {\it the cake} herself. 
\label{fig:bridging}
\caption{\nss{need a caption here}}
\end{figure}

Within the non anaphoric category, the annotators decide whether the NP denotes a unique NP, 
a nonunique NP, or a generic NP. For the nongeneric cases, further decisions are made based on 
whether the entity is hearer-old or -new, and if it is old, what kind of situation makes it old. 
For generics also, there are two categories depending on whether they appear 
with kind-level or individual-level predicates resulting in different properties of these NPs. 
Besides these, there are a few non-referential categories as well.

Besides the annotation categories, a few other decisions were also taken regarding annotations. 
For example, corresponding to the basic anaphora cases, the anaphoric links can be established with antecedent NPs 
in previous sentences as well as with NPs within the same sentence. 
Also to determine whether an NP gets a \llbl{Same\_head} or a \llbl{Different\_head} label, 
we refer to the closest antecedent even if it appears within the same sentence. 
Regarding annotations of nested NPs, we assume that these NPs are interpreted inside out. 
This helps determine the cases of \llbl{Bridging\_RestrictiveModifier} category. 
If an NP consists of a modifier that restricts its reference enough 
to make the referent identifiable to the addressees, the embedding NP is annotated with 
the \llbl{Bridging\_RestrictiveModifier} label. 


\Cref{fig:excerpt} is an excerpt from the ``Little Red Riding Hood'' document demonstrating 
the above-mentioned decisions regarding annotations and illustrating some of the categories from our annotation scheme.


\begin{figure*}[t]\small
\textit{Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child.}
\\[5pt]
Once \glosst{she}{\llbl{Same\_head}} gave \glosst{her}{\llbl{Different\_head}} 
\glosst{a little riding hood of \glosst{red velvet}{\llbl{Other\_nonreferential}}}{$\underset{~}{\llbl{nonuniq\_Hearer\_new\_spec}}$}, 
which suited \glosst{her}{\llbl{Same\_head}} so well that \glosst{she}{\llbl{Same\_head}} 
would never wear \glosst{anything else}{\llbl{Quantified}}; so \glosst{she}{\llbl{Same\_head}} was always called 
`\glosst{Little Red Riding Hood}{\llbl{uniq\_Hearer\_new}}.'
\caption{An annotated sentence from ``Little Red Riding Hood.'' The previous sentence is shown for context.}
\label{fig:excerpt}
\end{figure*}






 
%\ab{A brief discussion of the Annotation scheme}
\nss{we should probably have a name for the scheme---something like ``Functions of Definiteness Across Languages (FDAL)''? and a name for the categories (I have been calling them semantic functions).}\ab{I actually like the name ``Functions of Definiteness Across Languages'' but am not sure how to incorporate it here.}
\nss{suggestion: lead with a discourse excerpt, ideally illustrating a few of the categories, a nested NP, and 
an anaphoric link across sentence boundaries. I can help format it.}

\nss{make sure to cite related computational stuff such as \citep{reiter10}}

Examples from \citet[pp. 6--7]{croft-03}:

\eenumsentence{\small
\item He went to \textbf{the bank}. (def.)\\
	  Il est all\'{e} \`{a} \textbf{la banque}. (def.)
\item He showed \textbf{extreme care}. (unmarked)\\
	  Il montra \textbf{un soin extr\^{e}me}. (indef.)
\item I love \textbf{artichokes} and asparagus. (unmarked)
	  J'aime \textbf{les artichauts} et les asperges. (def.)
\item His brother became \textbf{a soldier}. (indef.)\\
	  Son fr\`{e}re est devenu \textbf{soldat}. (unmarked)
}
In the scheme used here, these should receive labels \llbl{nonuniq\_Hearer\_new\_spec}, \llbl{Other\_Nonreferential}, 
\llbl{Generic\_individualLevel}, and \llbl{Predicative\_equative\_role}, respectively.

\begin{figure*}\small
\begin{tabular}{p{.45\textwidth}p{.45\textwidth}}
\begin{itemize}
\item    \ilbl{Nonanaphora} $[-A,-B]$ \textbf{(00)}
  \begin{itemize}
  \item      \ilbl{Unique} $[+U]$ \textbf{(00)}
    \begin{itemize}
    \item        \ilbl{uniq\_Hearer\_old} $[-G,+O,+S]$ \textbf{(00)}
      \begin{itemize}
      \item          \llbl{uniq\_Physical\_copresence} $[+R]$ (00)
      \item          \llbl{uniq\_Larger\_situation} $[+R]$ (00)
      \item          \llbl{uniq\_predicative\_identity} $[+P]$ (00)
      \end{itemize}
    \item        \llbl{uniq\_Hearer\_new} $[-O]$
    \end{itemize}
   \item     \ilbl{Nonunique} $[-U]$
     \begin{itemize}
     \item       \ilbl{nonuniq\_Hearer\_old} $[+O]$
       \begin{itemize}
       \item         \llbl{nonuniq\_Physical\_copresence} $[-G,+R,+S]$
       \item         \llbl{nonuniq\_Larger\_situation} $[-G,+R,+S]$
       \item         \llbl{nonuniq\_predicative\_identity} $[+P]$
       \end{itemize}
     \item       \llbl{nonuniq\_Hearer\_new\_spec} $[-G,-O,+R,+S]$
     \item       \llbl{nonuniq\_nonspec} $[-G,-S]$
     \end{itemize}
   \item \ilbl{Generic} $[+G,-R]$
     \begin{itemize}
	   \item      \llbl{Generic\_kindLevel}
	   \item      \llbl{Generic\_individualLevel}
     \end{itemize}
  \end{itemize}
\end{itemize} &
\begin{itemize}
\item    \ilbl{Anaphora} $[+A]$
  \begin{itemize}
  \item      \ilbl{Basic} $[+O,-B]$
    \begin{itemize}
    \item        \llbl{Same\_head}
    \item        \llbl{Different\_head}
    \end{itemize}
  \item  \ilbl{Extended} $[+B]$
    \begin{itemize}
    \item        \llbl{Bridging\_nominal} $[-G,+R,+S]$
    \item        \llbl{Bridging\_event} $[+R,+S]$
    \item        \llbl{Bridging\_restrictiveModifier} $[-G,+S]$
    \item        \llbl{Bridging\_subtype\_instance} $[-G]$
    \item        \llbl{Bridging\_OtherContext} $[+O]$
    \end{itemize}
  \end{itemize}
\item \ilbl{Miscellaneous} $[-R]$
  \begin{itemize}
	\item \llbl{Pleonastic} $[-B,-P]$
	\item \llbl{Quantified}
	\item \llbl{Predicative\_equative\_role} $[-B,+P]$
	\item \llbl{Part\_of\_noncompositional\_mwe}
	\item \llbl{Measure\_Nonreferential}
	\item \llbl{Other\_Nonreferential}
  \end{itemize}
\end{itemize}
\end{tabular}
\caption{Taxonomy of definiteness functions, with number of occurrences in the training data \nss{TODO}. 
Internal (non-leaf) labels are in bold;\costversion{\nss{}} these are not annotated or predicted.\nss{TODO: normalize capitalization}
$+$/$-$ values are shown for ternary attributes \uline{A}naphoric, \uline{B}ridging, \uline{G}eneric, Hearer-\uline{O}ld, \uline{P}redicative, \uline{R}eferential, \uline{S}pecific, and \uline{U}nique; 
these are inherited from supercategories, but otherwise default to $0$.
Thus, for example, the full attribute specification for \llbl{uniq\_Physical\_copresence} is $[-A,-B,-G,+O,0P,+R,+S,+U]$.}
\label{fig:hierarchy}
\end{figure*}

\section{Data}\label{sec:data}

We used the definiteness corpus \cite{bhatia14} which consisted of texts from a few genres, namely TED talks, published news articles, speech (Presidential inauguration speech) and fictional narratives in English. However, currently majority of the corpus consists of the TED talks’ annotations (about 72\% of the data) with 18\% of speech and 10\% of news and fiction narratives data. We used a total of 812 sentences (a total of 20655 words), with 2950 NPs (the annotatable units). \cite{bhatia14} reports an inter annotator agreement of Cohen's Kappa = 0.94 within the TED genre and 0.91 for combined genres on a previous version of the annotation scheme.  


\section{Classification framework}\label{sec:modeling}

To model the relationship between the grammar of definiteness and its semantic functions in a data-driven fashion,
we work within the supervised framework of feature-rich discriminative classification, 
treating the functional categories from \cref{sec:scheme} as output labels $y$
and various lexical, morphological, and syntactic characteristics of the language as features of the input $x$.
Specifically, we learn a probabilistic log-linear model similar to multiclass logistic regression, 
but deviating in that
logistic regression treats each output label (response) as atomic, whereas 
we decompose each into \emph{attributes} based on their linguistic definitions, 
enabling commonalities between related labels to be recognized.
Each weight in the model corresponds to a feature that mediates between 
\emph{percepts} (characteristics of the input noun phrase) and attributes (characteristics of the label).
\costversion{\nss{}the following ways:
\begin{itemize}
  \item Logistic regression treats each output label (response) as atomic; 
  we decompose each into \emph{attributes} based on their linguistic definitions, 
  enabling commonalities between related labels to be recognized.
  Each weight in the model corresponds to a feature that mediates between 
  \emph{percepts} (characteristics of the input noun phrase) and attributes (characteristics of the label).
  \item Logistic regression assumes a prediction is either correct or incorrect.
  We incorporate a \emph{cost function} that gives partial credit during learning when a related label 
  is predicted, so the learned model will better match our evaluation measure.
  \item Logistic regression assumes the space of possible predictions matches 
  the space of labels observed in the training data; we allow more abstract labels to be predicted, 
  which can receive partial credit. The scoring scheme encourages the predictor to ``back off'' 
  to a coarser label if it is not sufficiently confident about a fine-grained label.
\end{itemize}
These decisions are\nss{}}
This is aimed at attaining better predictive accuracy 
as well as feature weights that better describe the form--function interactions we are interested in recovering.

Our setup is formalized below, where we discuss the mathematical model and linguistically motivated features.

\subsection{Model}

At test time, we model the probability of semantic label $y$ 
conditional on a \nss{gold?} noun phrase $x$ as follows:
\begin{equation}
p_{\boldsymbol{\theta}}(y | x) = \log{\frac{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y)}}{\sum_{y' \in \mathcal{Y}}\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y')}}}
\end{equation}
where $\boldsymbol{\theta} \in \mathbb{R}^d$ is a vector of parameters (feature weights), and 
$\mathbf{f}: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^d$ is the feature function over input--label pairs.
The feature function is defined as follows:
\begin{equation}
\mathbf{f}(x,y) = \boldsymbol{\phi}(x) \times \tilde{\boldsymbol{\omega}}(y)
\end{equation}
where the percept function $\boldsymbol{\phi}: \mathcal{X} \rightarrow \mathbb{R}^c$ 
produces a vector of real-valued characteristics of the input, and  
the attribute function \fta{define a and c to make it clear} $\tilde{\boldsymbol{\omega}}: \mathcal{Y} \rightarrow \{0,1\}^a$
encodes characteristics of each label.
There is a feature for every percept--attribute pairing: so
$d = c \cdot a$ and $f_{(i-1)a+j}(x,y) = \phi_i(x)\tilde{\omega}_j(y), 1 \leq i \leq c, 1 \leq j \leq a$.
The contents of the percept and attribute functions are detailed in \cref{sec:attrs,sec:feats}.

For prediction, having learned weights $\hat{\boldsymbol{\theta}}$ we choose the $y$ that maximizes this probability:
\begin{equation}
\hat{y} \leftarrow \arg\max_{y' \in \mathcal{Y}} p_{\hat{\boldsymbol{\theta}}}(y | x)
\end{equation}

Training optimizes $\hat{\boldsymbol{\theta}}$ so as to maximize a convex $L_1$-regularized
\costversion{\emph{softmax-margin} }learning objective\costversion{ \citep{gimpel}} over the training data $\mathcal{D}$:
\begin{align}
\hat{\boldsymbol{\theta}} &= \argmax_{\boldsymbol{\theta}} L(\boldsymbol{\theta}, \mathcal{D}) \\
\begin{split}
L(\boldsymbol{\theta}, \mathcal{D}) &= -\lambda ||\boldsymbol{\theta}||_1 \\ 
+ \sum_{\langle x,y \rangle\in\mathcal{D}} &\log{\frac{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x,y)}}{\sum_{y' \in \mathcal{Y}}\exp{\left(\boldsymbol{\theta}^{\top}\mathbf{f}(x,y')\costversion{ + \kappa\textit{cost}(y,y')}\right)}}}
\end{split}
\end{align}
\costversion{The \emph{cost function} allows us to penalize some errors more than others during training, 
taking into account the linguistic functions of the labels.
It is zero for the gold label and nonnegative for the others.\nss{intuition}

This framework gives us several ways to design a classifier appropriate to the task: 
the attributes, the space of labels $\mathcal{Y}$ to consider, 
and the cost function, and of course, the features themselves.}
With \costversion{$\kappa = 0$, $\mathcal{Y} = \{\textit{gold labels in training}\}$, 
and }$\tilde{\boldsymbol{\omega}}(y) = \textit{the identity of the label}$, 
this reduces to standard logistic regression.

\subsection{Percepts}\label{sec:feats}

The characteristics of the input that are incorporated in the model, which we call \emph{percepts} 
to distinguish them from model features linking inputs to outputs,\footnote{See above.\fta{see section or subsection number?} 
are intended to capture the aspects of English morphosyntax that may be relevant 
to the semantic and pragmatic functions of definiteness.

After preprocessing the text with a dependency parser and coreference resolver, 
we extract the several kinds of percepts for each noun phrase (NP).

\subsubsection{Basic}

\paragraph{Words of interest.} 
These are the \emph{head} within the NP, all of its \emph{dependents}, and its \emph{governor} (external to the NP). 
We are also interested in the \emph{attached verb}, which is the first verb one encounters when traversing the dependency path upward from the head. 
For each of these words, we have separate percepts capturing: the token, the part-of-speech (POS) tag, the lemma, 
the dependency relation,
and (for the head only) a binary indicator of plurality (determined from the POS tag).
As there may be multiple dependents, we have additional features specific to the first and the last one. 
Moreover, to better capture tense, aspect and modality, we collect the attached verb's \emph{auxiliaries}. 
We also make note of \emph{neg} if it is attached to the verb.
    
\paragraph{Structural.} 
These are: the \emph{path length} from the head up to the root, and to the attached verb. 
We also have percepts for the number of dependents, and the number of dependency relations that link non-neighbors.
Integer values were binarized with thresholding.

\paragraph{Positional.} 
The \emph{token length} of the NP; the NP's \emph{location} in the sentence (first or second half); 
the \emph{attached verb's position} relative to the head (left or right). 
12~additional percept templates record the POS and lemma of the left and right neighbors of the head, governor, and attached verb.

\subsubsection{Contextual NPs}

When extracting features for a given NP (call it the ``target''), 
we also consider NPs in the following relationship with the target NP:
its \emph{immediate parent}, which is the smallest NP whose span fully subsumes that of the target; 
the \emph{immediate child}, which is the largest NP subsumed within the target;
the \emph{immediate precedent} and \emph{immediate successor} within the sentence; 
and the \emph{nearest preceding coreferent mention}.

For each of these related NPs, we include all of their basic percepts 
conjoined with the nature of the relation to the target.

\subsection{Attributes}\label{sec:attrs}

As noted above, though labels are organized into a tree hierarchy, 
there are actually several dimensions of commonality that suggest different groupings.
These attributes are encoded as ternary characteristics; 
for each label (including internal labels), every one of the 8 attributes  
is assigned a value of $+$, $-$, or $0$ (refer to \cref{fig:hierarchy}).
In order to capture these similarities in the model's features\costversion{ and cost function}, 
we define the attribute vector function $\boldsymbol{\omega}(y) = $
\begin{equation*}
[y, A(y), B(y), G(y), O(y), P(y), R(y), S(y), U(y)]^{\top}
\end{equation*}
where $A: \costversion{\mathcal{L} \cup \mathcal{I}\nss{}}\mathcal{Y} \rightarrow \{+, -, 0\}$ returns the value for Anaphoric, 
$B(y)$ for Bridging, etc. \fta{explain the three possible values: positive negative or 0}The identity of the label is also included in the vector so that 
different labels are always recognized as different by the attribute function.
The categorical components of this vector are then binarized to form $\tilde{\boldsymbol{\omega}}(y)$; 
however, instead of a binary component that fires for the $0$ value of each ternary attribute, 
there is a component that fires for \emph{any} value of the attribute---a sort of bias term.
The weights assigned to features incorporating $+$ or $-$ attribute values, then, 
are easily interpreted as deviations relative to the bias.

\costversion{\subsection{Cost and label space}\label{sec:cost}

The definiteness function hierarchy presented in \cref{fig:hierarchy} 
consists of 24~\emph{leaf labels}, which will be denoted $\mathcal{L}$, and 
10~more abstract \emph{intermediate labels}, denoted $\mathcal{I}$.
All of the gold labels in the training data are from $\mathcal{L}$, 
but we give our model the option to predict more abstract labels 
to receive partial credit.
We will therefore use $\mathcal{Y} = \mathcal{L} \cup \mathcal{I}$.

The relatedness of label pairs depends on the number of values that differ between the two original attribute vectors: 
let $\Delta(y,y') = |\boldsymbol{\omega}(y) \ominus \boldsymbol{\omega}(y')|$.\footnote{By the symmetric difference of two attribute vectors, we mean the subset of components that lack a matching (categorical) value.} 
For a gold leaf label $\ell$ and an internal label $\iota$ that subsumes it in the hierarchy 
(i.e., $\iota$ is an ancestor of $\ell$), let $\delta(\ell,\iota) =$ the distance in the hierarchy 
between $\ell$ and $\iota$.\footnote{There is no need to define $\delta(\iota, \cdot)$, as the training set does not contain intermediate labels.}
Now define $\textit{cost}(y,y') = \Delta(y,y') - 0.5^{\delta(y,y')}$: 
this assigns lower (better) cost to an internal label than to an incorrect leaf label with an equivalent attribute distance.}

\section{Evaluation}\label{sec:eval}

The following measures will be used to evaluate our predictor against the gold standard 
for the held-out evaluation (dev or test) set $\mathcal{E}$:
\begin{itemize}
  \item \textbf{Exact match:} This accuracy measure gives credit only where the predicted and gold labels 
  are identical.\costversion{\nss{} When the model is allowed to predict internal labels, we will report 
  overall precision and recall of leaf labels. Otherwise, we report accuracy.}
  \item \textbf{By leaf label:} We also compute precision and recall of each leaf label 
  to determine which categories are reliably predicted.
  \item \textbf{Soft match:} This accuracy measure gives partial credit where the predicted and gold labels 
  are related. It is computed as the proportion of attributes whose (categorical) values match: 
  $|\boldsymbol{\omega}(y) \cap \boldsymbol{\omega}(y')|/9$.
  \item \textbf{Perplexity:} This determines how ``surprised'' our model is by the gold labels 
  in the test set; the greater the probability mass assigned to the true labels, 
  the higher the score. 
  It is computed as $2^{\left(\sum_{\langle x, y \rangle \in \mathcal{E}} \log_2 p_{\hat{\boldsymbol{\theta}}}(y | x)\right) / |\mathcal{E}|}$.
\end{itemize}

\section{Experiments}\label{sec:exp}

\subsection{Experimental Setup}

The annotated corpus of \citet{bhatia14} (\cref{sec:data}) contains 16~documents 
in 3~genres: 12~prepared speeches (mostly TED talks), 2~newspaper articles, 
and 2~fictional narratives. We arbitrarily choose some documents to hold out from each genre; 
the resulting test set consists of 2~TED talks (``Alisa\_News'', ``RobertHammond\_park''), 
1~newspaper article (``crime1\_iPad\_E''), and 1~narrative (``Little Red Riding Hood'').
The test set then contains 3,558~tokens (110~sentences), in which there are 492~annotated NPs; 
while the training set contains 2,458~NPs among 17,097~tokens (702~sentences).
Gold NP boundaries are assumed throughout our experiments.

We use an in-house implementation of supervised learning with $L_1$-regularized AdaGrad \citep{adagrad}. 
Hyperparameters are tuned on a dev set formed by holding out every tenth instance from the training set 
(test set experiments use the full training set).\nss{early stopping? what exactly is tuned? optimize soft match acc?} 
Automatic dependency parses and coreference information were obtained with 
the parser and coreference resolution system in Stanford CoreNLP v.~3.3.0 \citep{socher-13,recasens-13}
for use in features (\cref{sec:feats}).

\subsection{Results}

\begin{table*}\small\centering
\begin{tabular}{lrrrr}
\multicolumn{1}{c}{\bf Condition} & \multicolumn{1}{c}{$|\boldsymbol{\theta}|$} & \multicolumn{1}{c}{\bf Exact Match Accuracy} & 
\multicolumn{1}{c}{\bf Soft Match Accuracy} & \multicolumn{1}{c}{\bf Perplexity} \\
\midrule
Majority baseline & --- & \\
Log-linear classifier, no grouping by attributes & & \\
Full log-linear classifier & & \\
\end{tabular}
\label{tbl:results}
\caption{Classifier versus baselines.}
\end{table*}

\nss{English: ±cost function, ±non-identity attributes, ±predicting intermediate labels}

\nss{maybe: which attribute groupings produce the best classifier, if we want to force a hierarchy}

\nss{feature/attribute ablations}

\nss{Hindi?}

\section{Related Work}

\nss{computational approaches to things related to definiteness, e.g. in MT. also would be good to mention Bresnan's work 
on predicting syntactic alternations with logistic regression (here we want to predict the hidden information 
so that the classifier is useful for applications!).}

\section{Conclusion}\label{sec:conclusion}





\bibliographystyle{aclnat}
% you bib file should really go here
\setlength{\bibsep}{10pt}
{\fontsize{10}{12.25}\selectfont
\bibliography{definiteness}}


\end{document}


\section{Discussion: Error analysis, comparison across languages}\label{sec:discussion}

\ab{confusion matrix - Hindi vs.~Eng (for common annotations)- semantic annotations
differences in annotations- why- different grammatical constructions, e.g. NP in 1 language but predicate in another etc.
what do similarities tell us about grammaticalization, what do differences tell.
discuss- how this analysis can help in MT}

%A total of 6 annotators contributed to the annotations. Two of the annotators are linguists while others are computer scientists working on languages and an undergraduate student of Linguistics. Two annotators annotated most of the data, the inter annotator agreement between them was very high with Cohen's Kappa score of 0.94 within the TED corpus and 0.91 for combined genres.  However the other 4 annotators also annotated some of the data, but one of the two annotators with most experience took these data as inputs and revised annotations according to their annotations. This two-step process of annotation was taken to increase the amount of data within a short period of time and also to achieve consistency within annotations. All annotators were trained using the annotation guidelines and example annotated texts. Then the annotators were asked to annotate some data, these annotations were discussed to reach at consensus. The annotations used for discussions to reach at consensus were not included while calculating the inter annotator agreement.

\ab{description of data, IAA, other statistics- n(annotations), n(annotations/ category)
classifiers for English- baseline, accuracy etc, confusion matrix- classifier predictions and gold standard
may be a brief discussion of feature ablation study-  which features are most helpful at least in these languages corresponding to individual categories (a clue to grammaticalization)
Principle component analysis, clustering of features}


